{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_gensim/blob/master/Exemplo_PMI_pt_br.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IziPWVaeGlWk"
      },
      "source": [
        "# Exemplo LDA Tópicos coerentes usando Gensim em pt-br\n",
        "\n",
        "https://python.plainenglish.io/collocation-discovery-with-pmi-3bde8f351833\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "##Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "outputs": [],
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "outputs": [],
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RufkKnojlwzu"
      },
      "source": [
        "# 1 - Instalação do spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LeiOTx0Dlk"
      },
      "source": [
        "https://spacy.io/\n",
        "\n",
        "Modelos do spaCy para português:\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYSkCUy-Dsdy",
        "outputId": "9b3f6c12-dea0-4526-d23c-d5e0a91d5fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (65.5.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.38.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Instala o spacy\n",
        "!pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Fvx0TVRQUw",
        "outputId": "a9230614-aa1d-49a9-8cb3-4fdb4f184a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy==3.2.0 in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.6.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (65.5.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.7.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.0) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Instala uma versão específica\n",
        "!pip install -U spacy==3.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GwcgkOlWi3"
      },
      "source": [
        "Realiza o download e carrega os modelos necessários a biblioteca\n",
        "\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z4LqE5kTwDYm"
      },
      "outputs": [],
      "source": [
        "# Definição do nome do arquivo do modelo\n",
        "#ARQUIVOMODELO = \"pt_core_news_sm\"\n",
        "#ARQUIVOMODELO = \"pt_core_news_md\"\n",
        "ARQUIVOMODELO = \"pt_core_news_lg\"\n",
        "\n",
        "# Definição da versão da spaCy\n",
        "VERSAOSPACY = \"-3.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aJ2KB3UCp-ws"
      },
      "outputs": [],
      "source": [
        "#Baixa automaticamente o arquivo do modelo.\n",
        "#!python -m spacy download {ARQUIVOMODELO}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASk5iFeUp9LE",
        "outputId": "88c76d4c-eb5d-4bf0-90e7-cc62e80536f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-09 23:54:32--  https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.2.0/pt_core_news_lg-3.2.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/fcaf57f0-07de-4dbc-9419-3b54eb2651b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221109%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221109T235432Z&X-Amz-Expires=300&X-Amz-Signature=e463697196762575378fab513616a19a26d86165b291faf712b04632ab38918e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-3.2.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-11-09 23:54:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/fcaf57f0-07de-4dbc-9419-3b54eb2651b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221109%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221109T235432Z&X-Amz-Expires=300&X-Amz-Signature=e463697196762575378fab513616a19a26d86165b291faf712b04632ab38918e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-3.2.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577393393 (551M) [application/octet-stream]\n",
            "Saving to: ‘pt_core_news_lg-3.2.0.tar.gz.2’\n",
            "\n",
            "pt_core_news_lg-3.2 100%[===================>] 550.64M  92.3MB/s    in 6.2s    \n",
            "\n",
            "2022-11-09 23:54:38 (88.7 MB/s) - ‘pt_core_news_lg-3.2.0.tar.gz.2’ saved [577393393/577393393]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Realiza o download do arquivo do modelo para o diretório corrente\n",
        "!wget https://github.com/explosion/spacy-models/releases/download/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_LkF7Nfm8_"
      },
      "source": [
        "Descompacta o arquivo do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9fCQQJGeVEY",
        "outputId": "f192488e-4620-4c26-9fbe-237dae3de96d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_core_news_lg-3.2.0/\n",
            "pt_core_news_lg-3.2.0/LICENSE\n",
            "pt_core_news_lg-3.2.0/LICENSES_SOURCES\n",
            "pt_core_news_lg-3.2.0/MANIFEST.in\n",
            "pt_core_news_lg-3.2.0/PKG-INFO\n",
            "pt_core_news_lg-3.2.0/README.md\n",
            "pt_core_news_lg-3.2.0/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/__init__.py\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/LICENSE\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/LICENSES_SOURCES\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/README.md\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/accuracy.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/attribute_ruler/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/attribute_ruler/patterns\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/config.cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/lookups/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/lookups/lookups.bin\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/moves\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/moves\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tokenizer\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/key2row\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/lookups.bin\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/strings.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/vectors\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/vectors.cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/PKG-INFO\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/SOURCES.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/dependency_links.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/entry_points.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/not-zip-safe\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/requires.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/top_level.txt\n",
            "pt_core_news_lg-3.2.0/setup.cfg\n",
            "pt_core_news_lg-3.2.0/setup.py\n"
          ]
        }
      ],
      "source": [
        "# Descompacta o arquivo do modelo\n",
        "!tar -xvf  /content/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ovOx-3Wb-JJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0266a284-b3c9-447d-ba36-7c8207e724b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot move '/content/pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0' to '/content/pt_core_news_lg/pt_core_news_lg-3.2.0': Directory not empty\n"
          ]
        }
      ],
      "source": [
        "# Coloca a pasta do modelo descompactado em uma pasta de nome mais simples\n",
        "!mv /content/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}/{ARQUIVOMODELO}{VERSAOSPACY} /content/{ARQUIVOMODELO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHT2c89qvwK"
      },
      "source": [
        "Carrega o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nbELnrpgA4T1"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas.\n",
        "import spacy\n",
        "\n",
        "CAMINHOMODELO = \"/content/\" + ARQUIVOMODELO\n",
        "\n",
        "nlp = spacy.load(CAMINHOMODELO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTTdqxKQ1Ay"
      },
      "source": [
        "Recupera os stopwords do spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OBInu7ayQ31J"
      },
      "outputs": [],
      "source": [
        "# Recupera as stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_EYNu-_RX7k"
      },
      "source": [
        "Lista dos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUSaUJEWRbnZ",
        "outputId": "c613757d-19e8-4ec1-b26f-d42994674a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de stopwords: 416\n",
            "{'porém', 'outras', 'cedo', 'tais', 'sétima', 'fazemos', 'parte', 'povo', 'ali', 'vão', 'e', 'põe', 'aí', 'porquanto', 'aos', 'pôde', 'adeus', 'esta', 'qualquer', 'primeira', 'eu', 'está', 'ela', 'desta', 'diz', 'apoia', 'foste', 'quieta', 'tivemos', 'sem', 'essa', 'grande', 'fomos', 'muito', 'terceira', 'fazeis', 'caminho', 'iniciar', 'vai', 'ter', 'coisa', 'dentro', 'este', 'nove', 'tiveste', 'quanto', 'vindo', 'foi', 'minha', 'nos', 'quatro', 'aquele', 'embora', 'usar', 'fará', 'dezanove', 'fazia', 'apontar', 'estou', 'elas', 'cá', 'minhas', 'era', 'o', 'são', 'nada', 'pelas', 'seus', 'geral', 'fazem', 'esse', 'vinda', 'onde', 'onze', 'fez', 'mas', 'lado', 'outros', 'fostes', 'talvez', 'vens', 'meu', 'nesta', 'pois', 'momento', 'num', 'dizer', 'nosso', 'todos', 'no', 'estes', 'os', 'oito', 'boa', 'estivestes', 'fora', 'és', 'dão', 'me', 'porquê', 'na', 'das', 'ontem', 'novo', 'máximo', 'sabe', 'tempo', 'final', 'outra', 'diante', 'esses', 'direita', 'quando', 'algo', 'esteve', 'nossa', 'nossas', 'sim', 'tuas', 'dois', 'isso', 'nessa', 'meio', 'logo', 'pegar', 'desse', 'ponto', 'somos', 'mil', 'a', 'cuja', 'usa', 'vem', 'já', 'teus', 'daquele', 'aquela', 'estive', 'podia', 'demais', 'mês', 'quarto', 'sou', 'toda', 'puderam', 'tenho', 'contra', 'apoio', 'inclusive', 'vossos', 'alguns', 'desde', 'enquanto', 'parece', 'for', 'sétimo', 'nível', 'ambas', 'como', 'cada', 'à', 'valor', 'não', 'tentar', 'forma', 'naquela', 'ver', 'quinze', 'daquela', 'podem', 'tudo', 'estiveram', 'quê', 'sexta', 'bem', 'pontos', 'antes', 'conhecido', 'fim', 'por', 'eles', 'partir', 'muitos', 'quinto', 'neste', 'oitava', 'te', 'teve', 'tal', 'nenhuma', 'maiorias', 'fazer', 'grupo', 'número', 'conhecida', 'área', 'da', 'quais', 'favor', 'é', 'três', 'relação', 'sois', 'um', 'em', 'tiveram', 'possível', 'veja', 'sob', 'tivestes', 'vários', 'sistema', 'nuns', 'dar', 'dessa', 'duas', 'aqui', 'custa', 'pelo', 'aqueles', 'poderá', 'sua', 'mesmo', 'breve', 'deve', 'quero', 'vosso', 'eventual', 'vos', 'pode', 'tanta', 'além', 'sempre', 'às', 'quinta', 'porque', 'teu', 'uma', 'sete', 'que', 'estará', 'deverá', 'maior', 'obrigado', 'querem', 'tens', 'cujo', 'estás', 'tente', 'estar', 'dezoito', 'próxima', 'números', 'deste', 'ligado', 'seu', 'seis', 'devem', 'novas', 'umas', 'estava', 'então', 'vinte', 'nossos', 'ir', 'até', 'local', 'ora', 'fazes', 'vossas', 'mais', 'poder', 'lá', 'para', 'suas', 'dezasseis', 'ambos', 'apenas', 'certamente', 'nós', 'quem', 'segunda', 'nunca', 'lhe', 'faz', 'todas', 'ao', 'grandes', 'têm', 'contudo', 'de', 'naquele', 'tanto', 'mal', 'obrigada', 'uns', 'nem', 'pelos', 'dezassete', 'do', 'só', 'ou', 'falta', 'vais', 'posso', 'tarde', 'menor', 'portanto', 'temos', 'põem', 'cinco', 'último', 'tu', 'com', 'se', 'meus', 'sobre', 'próximo', 'catorze', 'ele', 'ainda', 'inicio', 'bastante', 'baixo', 'fui', 'quieto', 'aquelas', 'atrás', 'pouca', 'após', 'comprida', 'longe', 'você', 'debaixo', 'perto', 'estivemos', 'dizem', 'estado', 'oitavo', 'tentei', 'estiveste', 'cento', 'aquilo', 'foram', 'tão', 'vocês', 'quer', 'quarta', 'nova', 'depois', 'doze', 'bom', 'vêm', 'questão', 'irá', 'assim', 'tua', 'comprido', 'des', 'dos', 'certeza', 'novos', 'vez', 'vezes', 'faço', 'seria', 'numa', 'nas', 'primeiro', 'lugar', 'dá', 'todo', 'isto', 'ser', 'tive', 'agora', 'cima', 'conselho', 'acerca', 'nesse', 'vossa', 'exemplo', 'próprio', 'menos', 'dez', 'algumas', 'essas', 'pouco', 'terceiro', 'tentaram', 'tipo', 'tem', 'disso', 'maioria', 'somente', 'possivelmente', 'estão', 'qual', 'estas', 'vós', 'treze', 'entre', 'posição', 'pela', 'meses', 'corrente', 'através', 'zero', 'sei', 'sexto', 'saber', 'segundo', 'as', 'tendes', 'ademais', 'também'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Quantidade de stopwords:\", len(spacy_stopwords))\n",
        "\n",
        "print(spacy_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyEaXKeaLWlq"
      },
      "source": [
        "## getTokensSemStopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pbUf_V_1axS2"
      },
      "outputs": [],
      "source": [
        "def getTokensSemStopword(tokens, spacy_stopwords=spacy_stopwords):\n",
        "    \"\"\"\n",
        "      Retira os tokens da lista de tokens tokens que estão na lista de stopword.\n",
        "      A lista de tokens pode ou não estar dentro de uma outra lista.\n",
        "    \n",
        "      Parâmetros:\n",
        "        `tokens` - Uma lista com os tokens ou uma lista de lista de tokens.\n",
        "        `spacy_stopwords` - Uma lista com as stopword. \n",
        "    \"\"\"\n",
        "    \n",
        "    # Verifica se é uma lista de palavras(str) ou ou uma lista de lista\n",
        "    if type(tokens[0]) is str:\n",
        "      lista_tokens = [tokens]\n",
        "    else:\n",
        "      lista_tokens = tokens\n",
        "      \n",
        "    # Lista de retorno\n",
        "    lista_tokens_sem_stopwords = []  \n",
        "\n",
        "    # Percorre a lista de tokens\n",
        "    for texto in lista_tokens:\n",
        "\n",
        "      # Lista dos tokens sem as stopwords\n",
        "      tokens_sem_stopwords = []\n",
        "      \n",
        "      # Percorre os tokens    \n",
        "      for token in texto:\n",
        "        # Verifica se o toke não está na lista de stopwords para adicionar a nova lista\n",
        "        if token not in spacy_stopwords:\n",
        "          tokens_sem_stopwords.append(token)\n",
        "      \n",
        "      # Adiciona a lista de tokens sem stopwords na lista de retorno se tiver uma palavra\n",
        "      if len(tokens_sem_stopwords) != 0:\n",
        "        lista_tokens_sem_stopwords.append(tokens_sem_stopwords)\n",
        "\n",
        "    if type(tokens[0]) is str:      \n",
        "      return lista_tokens_sem_stopwords[0]\n",
        "    else:\n",
        "      return lista_tokens_sem_stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7XoLBuW6woe"
      },
      "source": [
        "## getSentencasTexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iR9Oc6Yf6zMa"
      },
      "outputs": [],
      "source": [
        "def getSentencasTexto(textos, nlp = nlp):\n",
        "\n",
        "  \"\"\"\n",
        "     Sentencia um texto ou uma lista de textos.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "      `nlp` - Modelo spacy carregado.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista dos tokens\n",
        "  lista_sentencas = []\n",
        "\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    # Sentencia o documento\n",
        "    doc = nlp(texto)\n",
        "    sentencas = []\n",
        "\n",
        "    # Percorre as sentenças do documento\n",
        "    for sentenca in doc.sents:   \n",
        "      sentencas.append(str(sentenca))\n",
        "\n",
        "    lista_sentencas = lista_sentencas + sentencas  \n",
        "\n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_sentencas[0]\n",
        "  else:\n",
        "    return lista_sentencas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5czwzaxKza0y"
      },
      "source": [
        "## getSentencasMinusculo\n",
        "\n",
        "Retorna a lista das sentencas do texto em minúsculo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MQQAO4Raza0z"
      },
      "outputs": [],
      "source": [
        "def getSentencasMinusculo(textos):\n",
        "\n",
        "  \"\"\"\n",
        "     Sentencia um texto ou uma lista de textos em minusculo.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista dos tokens\n",
        "  lista_sentencas = []\n",
        "\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    lista_sentencas.append(str(texto).lower())\n",
        "      \n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_sentencas[0]\n",
        "  else:\n",
        "    return lista_sentencas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGaf7bkpAEiX"
      },
      "source": [
        "## getTokensTexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gWxyAo54AOHU"
      },
      "outputs": [],
      "source": [
        "def getTokensTexto(textos, nlp = nlp):\n",
        "\n",
        "  \"\"\"\n",
        "     Tokeniza um texto ou uma lista de textos.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista de retorno\n",
        "  lista_tokens_texto = []\n",
        "\n",
        "  # Percorre a lista de texto\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    # Verifica se o sentenca não foi processado pelo spaCy  \n",
        "    if type(texto) is not spacy.tokens.doc.Doc:\n",
        "        # Realiza o parsing no spacy\n",
        "        doc = nlp(texto)\n",
        "    else:\n",
        "        doc = texto\n",
        "\n",
        "    # Lista dos tokens\n",
        "    lista_tokens = []\n",
        "\n",
        "    # Percorre a sentença adicionando os tokens\n",
        "    for token in doc:    \n",
        "      lista_tokens.append(token.text)\n",
        "    \n",
        "    # Adiciona a lista de tokens na lista de sentenças\n",
        "    lista_tokens_texto.append(lista_tokens)\n",
        "\n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_tokens_texto[0]\n",
        "  else:\n",
        "    return lista_tokens_texto"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## removerPontuacao"
      ],
      "metadata": {
        "id": "l3VOqrF8h3-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removerPontuacao(textos):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        \n",
        "        doc = nlp(\" \".join(texto)) \n",
        "\n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ not in ['PUNCT']:\n",
        "              sentenca.append(token.text)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "R5P_9zfFh3-y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## relevantes"
      ],
      "metadata": {
        "id": "2C4s2rvzJ7iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relevantes(textos, postags_permitidas=['VER', 'AUX', 'NOUN']):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        \n",
        "        doc = nlp(\" \".join(texto)) \n",
        "      \n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ in postags_permitidas:\n",
        "              sentenca.append(token.text)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "5F6PEOkZJ7iv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lematizacao"
      ],
      "metadata": {
        "id": "1WOT9a_X5dkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizacao(textos, postags_permitidas=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        doc = nlp(\" \".join(texto)) \n",
        "\n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ in postags_permitidas:\n",
        "              sentenca.append(token.lemma_)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "SbnNOPv85d0C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preparaCorpus"
      ],
      "metadata": {
        "id": "b32wPnBG1faQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import das biblitecas\n",
        "import pandas as pd\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "def preparaCorpus(textos,                   \n",
        "                  sentenciaTexto=False,\n",
        "                  tornaMinusculo=False,\n",
        "                  removePontuacao=False, \n",
        "                  removeStopwords=False, \n",
        "                  bigramas=False, \n",
        "                  trigramas=False,\n",
        "                  somenteRelevante=False,\n",
        "                  postag_relevante=['VERB', 'AUX', 'NOUN'],\n",
        "                  lematizar=False,                  \n",
        "                  postag_lema=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \n",
        "    # Verifica se é um textos é str ou uma lista de texto\n",
        "    if type(textos) is str:\n",
        "      # Sentencia o texto\n",
        "      lista_sentencas = [textos]\n",
        "    else:\n",
        "      lista_sentencas = textos\n",
        "    \n",
        "    # Converte o texto em uma lista de sentencas\n",
        "    if sentenciaTexto==True:\n",
        "      lista_sentencas = getSentencasTexto(lista_sentencas)\n",
        "\n",
        "    # Converte o texto em minúsuclo\n",
        "    if tornaMinusculo==True:\n",
        "      lista_sentencas = getSentencasMinusculo(lista_sentencas)\n",
        "    \n",
        "    # tokeniza o texto\n",
        "    lista_sentencas_palavras = getTokensTexto(lista_sentencas)\n",
        "\n",
        "    # Remove a pontuação \n",
        "    if removePontuacao==True:\n",
        "        lista_sentencas_palavras = removerPontuacao(lista_sentencas_palavras)        \n",
        "\n",
        "    # Remove as stop words\n",
        "    if removeStopwords==True:\n",
        "      lista_sentencas_palavras = getTokensSemStopword(lista_sentencas_palavras)\n",
        "\n",
        "    # Criar bigramas ou trigramas\n",
        "    if bigramas==True:\n",
        "      # Construa os modelos de bigramas\n",
        "      bigram = gensim.models.Phrases(lista_sentencas_palavras, min_count=5, threshold=100) # max_topicse mais alto menos frases.\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "      bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "      lista_sentencas_palavras = [bigram_mod[doc] for doc in lista_sentencas_palavras]\n",
        "    \n",
        "    if trigramas==True:      \n",
        "      # Construa os modelos de bigramas\n",
        "      bigram = gensim.models.Phrases(lista_sentencas_palavras, min_count=5, threshold=100) # max_topicse mais alto menos frases.\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "      bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "      # Construa os modelos de trigramas\n",
        "      trigram = gensim.models.Phrases(bigram[lista_sentencas_palavras], threshold=100)\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama    \n",
        "      trigram_mod = gensim.models.phrases.Phraser(trigram)   \n",
        "      lista_sentencas_palavras = [trigram_mod[bigram_mod[doc]] for doc in lista_sentencas_palavras]   \n",
        "    \n",
        "    # Somente palavras relevantes\n",
        "    if somenteRelevante==True:      \n",
        "      lista_sentencas_palavras = relevantes(lista_sentencas_palavras, postags_permitidas=postag_relevante)\n",
        "    \n",
        "    # Faça a lematização mantendo apenas para noun, adj, vb, adv\n",
        "    if lematizar==True:      \n",
        "      lista_sentencas_palavras = lematizacao(lista_sentencas_palavras, postags_permitidas=postag_lema)\n",
        "\n",
        "    return lista_sentencas_palavras"
      ],
      "metadata": {
        "id": "rSW4ign41h1L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxFiqbpPQ-CR"
      },
      "source": [
        "# 2 - Instalação do Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdjN6H6t_L08"
      },
      "source": [
        "Instalando o gensim no Google Colaboratory.\n",
        "\n",
        "No Jupiter Notebook executar através \"Anaconda Prompt\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGFVnIzQGrEH",
        "outputId": "d5285d46-6c36-41c2-f86e-e5b738e41e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U gensim\n",
        "#!pip install -U gensim==4.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPuCCLyuBIeZ"
      },
      "source": [
        "# Exemplos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Conjunto de dados\n"
      ],
      "metadata": {
        "id": "BZQFeIobYaYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9QEWUZkfiiwd"
      },
      "outputs": [],
      "source": [
        "CohQuAD_Coh = [\n",
        "# 20 Perguntas do CohQuAD Coerentes\n",
        "\"Como enfileirar elementos em uma fila?\",      \n",
        "\"Como desenfileirar elementos em uma fila?\",\n",
        "\"Como empilhar elementos em uma pilha?\",\n",
        "\"Como empilhar e desempilhar elementos em uma pilha?\",\n",
        "\"Como empilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"Como empilhar e desempilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"Como desempilhar elementos em uma pilha?\",\n",
        "\"Como desempilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"O que é uma pilha e como empilhar seu elemento?\",\n",
        "\"O que é uma fila e como enfileirar seu elemento?\",\n",
        "\"O que é uma fila e como desenfileirar um elemento nela?\",\n",
        "\"O que é uma pilha e como desempilhar um elemento nela?\",\n",
        "\"O que é uma fila e como enfileirar um elemento nela?\",\n",
        "\"O que é uma pilha e como empilhar um elemento nela?\",\n",
        "\"O que é uma pilha e como empilhar e desempilhar seus elementos?\",\n",
        "\"O que é uma fila e como enfileirar e desenfileirar seus elementos?\",\n",
        "\"Como são implementadas as operações de empilhar e desempilhar elementos em uma pilha?\",\n",
        "\"Como são implementadas as operações de enfileirar e desenfileirar elementos em uma fila?\",\n",
        "\"Em uma pilha a operação de empilhar ocorre em qual extremidade?\",\n",
        "\"Em uma fila a operação de enfileirar ocorre em qual extremidade?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EwF293tUiiwi"
      },
      "outputs": [],
      "source": [
        "CohQuAD_Inc = [\n",
        "# 20 Perguntas do CohQuAD Incoerentes\n",
        "\"Como enfileirar elementos em uma pilha?\",\n",
        "\"Como desenfileirar elementos em uma pilha?\",\n",
        "\"Como empilhar elementos em uma fila?\",\n",
        "\"Como empilhar e desempilhar elementos em uma fila?\",\n",
        "\"Como empilhar elementos em uma estrutura de dados fila?\",\n",
        "\"Como empilhar e desempilhar elementos em uma estrutura de dados fila?\",\n",
        "\"Como desempilhar elementos em uma fila?\",\n",
        "\"Como desempilhar elementos em uma estrutura de dados fila?\",\n",
        "\"O que é uma fila e como empilhar seu elemento?\",\n",
        "\"O que é uma pilha e como enfileirar seu elemento?\",\n",
        "\"O que é uma pilha e como desenfileirar um elemento nela?\",\n",
        "\"O que é uma fila e como desempilhar um elemento nela?\",\n",
        "\"O que é uma pilha e como enfileirar um elemento nela?\",\n",
        "\"O que é uma fila e como empilhar um elemento nela?\",\n",
        "\"O que é uma fila e como empilhar e desempilhar seus elementos?\",\n",
        "\"O que é uma pilha e como enfileirar e desenfileirar seus elementos?\",\n",
        "\"Como são implementadas as operações de empilhar e desempilhar elementos em uma fila?\",\n",
        "\"Como são implementadas as operações de enfileirar e desenfileirar elementos em uma pilha?\",\n",
        "\"Em uma pilha a operação de enfileirar ocorre em qual extremidade?\",\n",
        "\"Em uma fila a operação de empilhar ocorre em qual extremidade?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando o dicionário sem lematização e sem as stopwords\n",
        "\n",
        "textos = preparaCorpus(CohQuAD_Coh,                        \n",
        "                       sentenciaTexto=True,\n",
        "                       tornaMinusculo=True,\n",
        "                       removePontuacao=True,\n",
        "                       somenteRelevante=False,\n",
        "                       removeStopwords=False)"
      ],
      "metadata": {
        "id": "aJecgsByjHDu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Cálculo de PMI"
      ],
      "metadata": {
        "id": "U02pkc1ueo60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 1"
      ],
      "metadata": {
        "id": "xQGqu8jXjXe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens = len(textos[0])\n",
        "print(n_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxnoFaT_rhKH",
        "outputId": "7823e32c-eaaf-4377-d0c4-63b53a3449bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sem janela\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela\n",
        "def gerarBigramasJanela(data, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(data)):\n",
        "        janela = data[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama"
      ],
      "metadata": {
        "id": "a-246Qn3kc-5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textos_bgs = gerarBigramasJanela(textos[0])\n",
        "print(textos_bgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg2W1Eagks2m",
        "outputId": "35327a25-fb02-42e4-9624-47deb5914440"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['como', 'enfileirar'], ['enfileirar', 'elementos'], ['elementos', 'em'], ['em', 'uma'], ['uma', 'fila']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contarFrequenciaPalavra(palavra, doc):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq"
      ],
      "metadata": {
        "id": "RJ88iAagqX_X"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_w1 = contarFrequenciaPalavra(doc=textos[0],palavra='como')\n",
        "print(n_w1)\n",
        "n_w2 = contarFrequenciaPalavra(doc=textos[0],palavra='enfileirar')\n",
        "print(n_w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcTYoAI2qdoe",
        "outputId": "28abcbda-4515-4fdd-b5b3-fbb537f2aac9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "        if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "            freq = freq + 1\n",
        "    return freq"
      ],
      "metadata": {
        "id": "-meBRm9CsUvt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_w1_w2 = contaFrequenciaBigrama(w1='como', w2='enfileirar',bigrams=textos_bgs)\n",
        "print(n_w1_w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8HSK08_j0Nf",
        "outputId": "10ccd75b-c159-406e-9804-2cfc67c45722"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def pmi(P_x, P_y, P_xy):\n",
        "    return math.log2(P_xy / (P_x * P_y))\n",
        "\n",
        "p_w1 = probabilidade(n_w1, n_tokens)\n",
        "\n",
        "p_w2 = probabilidade(n_w2, n_tokens)\n",
        "\n",
        "p_w1_w2 = probabilidade(n_w1_w2, len(textos_bgs))\n",
        "\n",
        "r = pmi(p_w1, p_w2, p_w1_w2)\n",
        "\n",
        "print(\"pmi para como e enfileirar = \",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLNHw5Zgl433",
        "outputId": "f8f46557-007c-44b0-e1c8-c8dd69340af7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pmi para como e enfileirar =  2.8479969065549504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 2"
      ],
      "metadata": {
        "id": "gze2XHNx3S4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Sem janela\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela\n",
        "def gerarBigramasJanela(doc, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(doc)):\n",
        "        janela = doc[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama\n",
        "\n",
        "\n",
        "def contarFrequenciaPalavra(doc,palavra):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq\n",
        "\n",
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "      if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "        freq = freq + 1\n",
        "    return freq\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def calculaPMI(w1, w2, doc, tamanho_janela=2):\n",
        "    \n",
        "    # Quantidade tokens do texto\n",
        "    n_tokens = len(doc)\n",
        "    # print(\"n_tokens:\",n_tokens)\n",
        "    \n",
        "    # Conta as frequencias de w1 e w2\n",
        "    n_w1 = contarFrequenciaPalavra(doc,w1)\n",
        "    # print(\"n_w1:\",n_w1)\n",
        "    n_w2 = contarFrequenciaPalavra(doc,w2)    \n",
        "    # print(\"n_w2:\",n_w2)\n",
        "\n",
        "    # Gera os bigrammas do documento\n",
        "    # bgs = gerarBigramas(doc)\n",
        "    bgs = gerarBigramasJanela(doc,tamanho_janela)\n",
        "\n",
        "    # Conta a frequencia no brigrama\n",
        "    n_w1_w2 = contaFrequenciaBigrama(w1, w2 , bigrams=bgs)\n",
        "    print(\"n_w1_w2:\",n_w1_w2)\n",
        "    \n",
        "    # Calcula as probabilidades\n",
        "    p_w1 = probabilidade(n_w1, n_tokens)\n",
        "    # print(\"p_w1:\",p_w1)\n",
        "    p_w2 = probabilidade(n_w2, n_tokens)\n",
        "    # print(\"p_w2:\",p_w2)\n",
        "    # p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    # print(\"p_w1_w2:\",p_w1_w2)\n",
        "    # Para evitar divisão por 0 e log de 0\n",
        "    e = sys.float_info.min\n",
        "\n",
        "    r = math.log2(((p_w1_w2+e) / ((p_w1 * p_w2) + e)))\n",
        "    \n",
        "    return r"
      ],
      "metadata": {
        "id": "sDsQZQ4pyviQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo pmi"
      ],
      "metadata": {
        "id": "_1dXCwz7zQo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'como'\n",
        "w2 = 'enfileirar'\n",
        "r = calculaPMI(w1, w2, textos[0])\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxFdneuCz-UK",
        "outputId": "8d68d3ee-98af-451a-ce65-59b6f265d98f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_w1_w2: 1\n",
            "pmi para 'como' e 'enfileirar' = 2.584962500721156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simetria"
      ],
      "metadata": {
        "id": "1Tg-lCbzzS-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'enfileirar'\n",
        "w2 = 'como'\n",
        "r = calculaPMI(w1, w2, textos[0])\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF0Ostl9DjSG",
        "outputId": "6f5e7bb7-acc0-4690-a6f7-8bbcc001f5e3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_w1_w2: 1\n",
            "pmi para 'enfileirar' e 'como' = 2.584962500721156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo igual a\n",
        "https://www.listendata.com/2022/06/pointwise-mutual-information-pmi.html"
      ],
      "metadata": {
        "id": "iSII2aSvzWZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dado = ['this','is','a','foo','bar','bar','black','sheep','foo','bar','bar','black','sheep','foo','bar','bar','black','sheep','shep','bar','bar','black','sentence']\n",
        "print(dado)\n",
        "\n",
        "w1 = 'foo'\n",
        "w2 = 'bar'\n",
        "\n",
        "r = calculaPMI(w1, w2, dado)\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB878l99gfnx",
        "outputId": "5de39869-4e7f-4f49-a948-7f748418c20b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'foo', 'bar', 'bar', 'black', 'sheep', 'foo', 'bar', 'bar', 'black', 'sheep', 'foo', 'bar', 'bar', 'black', 'sheep', 'shep', 'bar', 'bar', 'black', 'sentence']\n",
            "n_w1_w2: 3\n",
            "pmi para 'foo' e 'bar' = 1.5235619560570128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Cálculo de NPMI\n",
        "\n",
        "Intervalo [-1 , 1]"
      ],
      "metadata": {
        "id": "iMp8LD_v6jvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 1"
      ],
      "metadata": {
        "id": "A-21Uckv6jvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela\n",
        "def gerarBigramasJanela(doc, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(doc)):\n",
        "        janela = doc[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama\n",
        "\n",
        "def contarFrequenciaPalavra(doc, palavra):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq\n",
        "\n",
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "        if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "        # if (bg[0] == w1 and bg[1] == w2):\n",
        "          freq = freq + 1\n",
        "    return freq\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def calculaNPMI(w1, w2, doc, tamanho_janela=2):\n",
        "    \n",
        "   # Quantidade tokens do texto\n",
        "    n_tokens = len(doc)\n",
        "    # print(\"n_tokens:\",n_tokens)\n",
        "    \n",
        "    # Conta as frequencias de w1 e w2\n",
        "    n_w1 = contarFrequenciaPalavra(doc,w1)\n",
        "    # print(\"n_w1:\",n_w1)\n",
        "    n_w2 = contarFrequenciaPalavra(doc,w2)    \n",
        "    # print(\"n_w2:\",n_w2)\n",
        "\n",
        "    # Gera os bigrammas do documento\n",
        "    # bgs = gerarBigramas(doc)\n",
        "    bgs = gerarBigramasJanela(doc,tamanho_janela)\n",
        "\n",
        "    # Conta a frequencia no brigrama\n",
        "    n_w1_w2 = contaFrequenciaBigrama(w1, w2 , bigrams=bgs)\n",
        "    # print(\"n_w1_w2:\",n_w1_w2)\n",
        "\n",
        "    # Calcula as probabilidades\n",
        "    p_w1 = probabilidade(n_w1, n_tokens)\n",
        "    # print(\"p_w1:\",p_w1)\n",
        "    p_w2 = probabilidade(n_w2, n_tokens)\n",
        "    # print(\"p_w2:\",p_w2)\n",
        "    # p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    # print(\"p_w1_w2:\",p_w1_w2)\n",
        "    # Para evitar divisão por 0 e log de 0\n",
        "    e = sys.float_info.min\n",
        "\n",
        "    # Para evitar divisão por 0 e log de 0\n",
        "    e = sys.float_info.min\n",
        "    \n",
        "    r = math.log2( (p_w1_w2  + e) / ((p_w1 * p_w2) + e))\n",
        "\n",
        "    nr = r / -(math.log2(p_w1_w2 + e))\n",
        "    return nr"
      ],
      "metadata": {
        "id": "6_r45HT36jvW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'como'\n",
        "w2 = 'enfileirar'\n",
        "r = calculaNPMI(w1, w2, textos[0])\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabf2fbc-294d-4dae-f7f3-a9e1cb05dcae",
        "id": "uJtR9-vi6jvX"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npmi para 'como' e 'enfileirar' = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'enfileirar'\n",
        "w2 = 'como'\n",
        "r = calculaNPMI(w2, w1, textos[0])\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWoueZ56DcSo",
        "outputId": "51b7c6d1-a5fc-482c-fa65-fa9f4b05d088"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npmi para 'enfileirar' e 'como' = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'como'\n",
        "w2 = 'elementos'\n",
        "r = calculaNPMI(w1, w2, textos[0])\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5w4J76aST4f",
        "outputId": "06ac3c79-a2f6-4000-a79c-c96e034d64b3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npmi para 'como' e 'elementos' = -0.9949413649692346\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhISBn2U5Tvb63aHqtR57G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}