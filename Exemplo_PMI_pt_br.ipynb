{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_gensim/blob/master/Exemplo_PMI_pt_br.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IziPWVaeGlWk"
      },
      "source": [
        "# Exemplo PMI em pt-br\n",
        "\n",
        "https://python.plainenglish.io/collocation-discovery-with-pmi-3bde8f351833\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "##Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "outputs": [],
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "outputs": [],
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RufkKnojlwzu"
      },
      "source": [
        "# 1 - Instalação do spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LeiOTx0Dlk"
      },
      "source": [
        "https://spacy.io/\n",
        "\n",
        "Modelos do spaCy para português:\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYSkCUy-Dsdy",
        "outputId": "c88cd1b6-3c8a-4107-d82e-988f63704d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (65.5.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.38.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Instala o spacy\n",
        "!pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Fvx0TVRQUw",
        "outputId": "30594928-2b2f-4d9c-807d-e1e2053b4c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy==3.2.0 in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.7.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (65.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2022.9.24)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.0) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Instala uma versão específica\n",
        "!pip install -U spacy==3.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GwcgkOlWi3"
      },
      "source": [
        "Realiza o download e carrega os modelos necessários a biblioteca\n",
        "\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z4LqE5kTwDYm"
      },
      "outputs": [],
      "source": [
        "# Definição do nome do arquivo do modelo\n",
        "#ARQUIVOMODELO = \"pt_core_news_sm\"\n",
        "#ARQUIVOMODELO = \"pt_core_news_md\"\n",
        "ARQUIVOMODELO = \"pt_core_news_lg\"\n",
        "\n",
        "# Definição da versão da spaCy\n",
        "VERSAOSPACY = \"-3.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aJ2KB3UCp-ws"
      },
      "outputs": [],
      "source": [
        "#Baixa automaticamente o arquivo do modelo.\n",
        "#!python -m spacy download {ARQUIVOMODELO}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASk5iFeUp9LE",
        "outputId": "9e256d8f-ab7e-4c09-b0b7-6939a69924d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-10 18:02:38--  https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.2.0/pt_core_news_lg-3.2.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/fcaf57f0-07de-4dbc-9419-3b54eb2651b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221110%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221110T180239Z&X-Amz-Expires=300&X-Amz-Signature=464ad3cba79a51c82248cc31af9c3dee14a75859d16f0faeb9c023dc50a84db5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-3.2.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-11-10 18:02:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/fcaf57f0-07de-4dbc-9419-3b54eb2651b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221110%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221110T180239Z&X-Amz-Expires=300&X-Amz-Signature=464ad3cba79a51c82248cc31af9c3dee14a75859d16f0faeb9c023dc50a84db5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-3.2.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577393393 (551M) [application/octet-stream]\n",
            "Saving to: ‘pt_core_news_lg-3.2.0.tar.gz.1’\n",
            "\n",
            "pt_core_news_lg-3.2 100%[===================>] 550.64M  9.61MB/s    in 69s     \n",
            "\n",
            "2022-11-10 18:03:48 (8.03 MB/s) - ‘pt_core_news_lg-3.2.0.tar.gz.1’ saved [577393393/577393393]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Realiza o download do arquivo do modelo para o diretório corrente\n",
        "!wget https://github.com/explosion/spacy-models/releases/download/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_LkF7Nfm8_"
      },
      "source": [
        "Descompacta o arquivo do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9fCQQJGeVEY",
        "outputId": "41b7bbc9-41d6-484f-a0c9-a74619de4b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_core_news_lg-3.2.0/\n",
            "pt_core_news_lg-3.2.0/LICENSE\n",
            "pt_core_news_lg-3.2.0/LICENSES_SOURCES\n",
            "pt_core_news_lg-3.2.0/MANIFEST.in\n",
            "pt_core_news_lg-3.2.0/PKG-INFO\n",
            "pt_core_news_lg-3.2.0/README.md\n",
            "pt_core_news_lg-3.2.0/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/__init__.py\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/LICENSE\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/LICENSES_SOURCES\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/README.md\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/accuracy.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/attribute_ruler/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/attribute_ruler/patterns\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/config.cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/lookups/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/lookups/lookups.bin\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/moves\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/moves\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tokenizer\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/key2row\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/lookups.bin\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/strings.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/vectors\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/vectors.cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/PKG-INFO\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/SOURCES.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/dependency_links.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/entry_points.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/not-zip-safe\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/requires.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/top_level.txt\n",
            "pt_core_news_lg-3.2.0/setup.cfg\n",
            "pt_core_news_lg-3.2.0/setup.py\n"
          ]
        }
      ],
      "source": [
        "# Descompacta o arquivo do modelo\n",
        "!tar -xvf  /content/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ovOx-3Wb-JJW"
      },
      "outputs": [],
      "source": [
        "# Coloca a pasta do modelo descompactado em uma pasta de nome mais simples\n",
        "!mv /content/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}/{ARQUIVOMODELO}{VERSAOSPACY} /content/{ARQUIVOMODELO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHT2c89qvwK"
      },
      "source": [
        "Carrega o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nbELnrpgA4T1"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas.\n",
        "import spacy\n",
        "\n",
        "CAMINHOMODELO = \"/content/\" + ARQUIVOMODELO\n",
        "\n",
        "nlp = spacy.load(CAMINHOMODELO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTTdqxKQ1Ay"
      },
      "source": [
        "Recupera os stopwords do spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OBInu7ayQ31J"
      },
      "outputs": [],
      "source": [
        "# Recupera as stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_EYNu-_RX7k"
      },
      "source": [
        "Lista dos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUSaUJEWRbnZ",
        "outputId": "2e38982a-c876-4f05-90f8-0d71ded9beac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de stopwords: 416\n",
            "{'essa', 'nesse', 'os', 'tarde', 'nosso', 'naquela', 'bom', 'nunca', 'ora', 'sexta', 'povo', 'essas', 'poder', 'contudo', 'tão', 'me', 'para', 'mas', 'puderam', 'ambos', 'vossas', 'outra', 'seis', 'contra', 'através', 'novo', 'três', 'seria', 'sem', 'pela', 'poderá', 'somente', 'é', 'teve', 'sempre', 'ligado', 'estiveste', 'ser', 'vosso', 'mil', 'cada', 'nada', 'pôde', 'tempo', 'oitavo', 'perto', 'enquanto', 'ponto', 'local', 'temos', 'mesmo', 'novas', 'direita', 'por', 'desse', 'nem', 'sobre', 'dessa', 'fostes', 'terceira', 'maioria', 'tudo', 'oito', 'e', 'o', 'nesta', 'vem', 'na', 'ter', 'também', 'lugar', 'tais', 'eles', 'segunda', 'estás', 'dos', 'fomos', 'forma', 'nível', 'quarto', 'daquela', 'mal', 'da', 'tanta', 'favor', 'aí', 'depois', 'relação', 'aquelas', 'das', 'terceiro', 'dez', 'até', 'segundo', 'deste', 'querem', 'vens', 'ao', 'grande', 'este', 'cima', 'bastante', 'sou', 'fazeis', 'suas', 'esse', 'deverá', 'adeus', 'porém', 'comprida', 'dão', 'meses', 'pouco', 'obrigada', 'nessa', 'logo', 'saber', 'tens', 'vezes', 'entre', 'conselho', 'coisa', 'nossos', 'nove', 'todo', 'próprio', 'valor', 'maior', 'te', 'porquanto', 'longe', 'diante', 'obrigado', 'ademais', 'porque', 'duas', 'era', 'quieto', 'onze', 'final', 'vai', 'algo', 'estivemos', 'após', 'meio', 'custa', 'quero', 'fará', 'lado', 'ela', 'quatro', 'são', 'oitava', 'dizem', 'nos', 'quinta', 'quieta', 'vais', 'próxima', 'lhe', 'numa', 'estar', 'estado', 'assim', 'tivemos', 'nova', 'eu', 'vós', 'acerca', 'muito', 'no', 'vêm', 'posso', 'sétimo', 'nossa', 'pelas', 'põem', 'não', 'cedo', 'vinda', 'dezassete', 'demais', 'desde', 'às', 'esteve', 'vossos', 'usa', 'em', 'mês', 'conhecida', 'sexto', 'estes', 'pois', 'ele', 'somos', 'portanto', 'estas', 'nenhuma', 'todas', 'outros', 'menos', 'pode', 'corrente', 'partir', 'certeza', 'onde', 'aqueles', 'seu', 'vindo', 'área', 'todos', 'que', 'está', 'iniciar', 'estava', 'dentro', 'nós', 'fazer', 'sua', 'quais', 'então', 'seus', 'baixo', 'têm', 'aos', 'teu', 'pegar', 'cuja', 'tive', 'primeiro', 'conhecido', 'bem', 'sistema', 'tentaram', 'fora', 'vez', 'caminho', 'daquele', 'cujo', 'agora', 'cento', 'geral', 'ontem', 'com', 'aquele', 'dar', 'pontos', 'sim', 'muitos', 'momento', 'esses', 'faz', 'lá', 'zero', 'primeira', 'fim', 'próximo', 'desta', 'elas', 'quinto', 'tal', 'as', 'nuns', 'quinze', 'a', 'minha', 'mais', 'número', 'ambas', 'algumas', 'pouca', 'estão', 'sei', 'fez', 'uma', 'só', 'atrás', 'como', 'ali', 'sétima', 'embora', 'eventual', 'ver', 'porquê', 'tipo', 'foram', 'nas', 'vão', 'nossas', 'outras', 'qual', 'tente', 'dá', 'parte', 'falta', 'boa', 'esta', 'fazem', 'tanto', 'tuas', 'aqui', 'um', 'tua', 'vos', 'talvez', 'és', 'dezoito', 'tem', 'quando', 'sete', 'usar', 'isto', 'quanto', 'grandes', 'exemplo', 'você', 'números', 'isso', 'irá', 'menor', 'estivestes', 'aquilo', 'tenho', 'for', 'quem', 'podem', 'podia', 'meus', 'máximo', 'apontar', 'antes', 'debaixo', 'novos', 'vinte', 'de', 'diz', 'certamente', 'pelo', 'foste', 'vossa', 'tu', 'alguns', 'inicio', 'cá', 'tentar', 'meu', 'último', 'parece', 'apenas', 'estou', 'foi', 'num', 'fui', 'dizer', 'questão', 'tiveste', 'apoia', 'minhas', 'sabe', 'sob', 'vários', 'já', 'breve', 'estive', 'quê', 'umas', 'deve', 'catorze', 'toda', 'naquele', 'veja', 'quarta', 'dezanove', 'posição', 'teus', 'vocês', 'fazia', 'dezasseis', 'des', 'à', 'qualquer', 'além', 'cinco', 'devem', 'doze', 'estiveram', 'se', 'sois', 'do', 'treze', 'maiorias', 'possível', 'põe', 'tentei', 'aquela', 'inclusive', 'neste', 'comprido', 'possivelmente', 'ir', 'pelos', 'tendes', 'ou', 'tivestes', 'estará', 'faço', 'fazes', 'grupo', 'ainda', 'quer', 'tiveram', 'apoio', 'uns', 'fazemos', 'disso', 'dois'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Quantidade de stopwords:\", len(spacy_stopwords))\n",
        "\n",
        "print(spacy_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyEaXKeaLWlq"
      },
      "source": [
        "## getTokensSemStopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pbUf_V_1axS2"
      },
      "outputs": [],
      "source": [
        "def getTokensSemStopword(tokens, spacy_stopwords=spacy_stopwords):\n",
        "    \"\"\"\n",
        "      Retira os tokens da lista de tokens tokens que estão na lista de stopword.\n",
        "      A lista de tokens pode ou não estar dentro de uma outra lista.\n",
        "    \n",
        "      Parâmetros:\n",
        "        `tokens` - Uma lista com os tokens ou uma lista de lista de tokens.\n",
        "        `spacy_stopwords` - Uma lista com as stopword. \n",
        "    \"\"\"\n",
        "    \n",
        "    # Verifica se é uma lista de palavras(str) ou ou uma lista de lista\n",
        "    if type(tokens[0]) is str:\n",
        "      lista_tokens = [tokens]\n",
        "    else:\n",
        "      lista_tokens = tokens\n",
        "      \n",
        "    # Lista de retorno\n",
        "    lista_tokens_sem_stopwords = []  \n",
        "\n",
        "    # Percorre a lista de tokens\n",
        "    for texto in lista_tokens:\n",
        "\n",
        "      # Lista dos tokens sem as stopwords\n",
        "      tokens_sem_stopwords = []\n",
        "      \n",
        "      # Percorre os tokens    \n",
        "      for token in texto:\n",
        "        # Verifica se o toke não está na lista de stopwords para adicionar a nova lista\n",
        "        if token not in spacy_stopwords:\n",
        "          tokens_sem_stopwords.append(token)\n",
        "      \n",
        "      # Adiciona a lista de tokens sem stopwords na lista de retorno se tiver uma palavra\n",
        "      if len(tokens_sem_stopwords) != 0:\n",
        "        lista_tokens_sem_stopwords.append(tokens_sem_stopwords)\n",
        "\n",
        "    if type(tokens[0]) is str:      \n",
        "      return lista_tokens_sem_stopwords[0]\n",
        "    else:\n",
        "      return lista_tokens_sem_stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7XoLBuW6woe"
      },
      "source": [
        "## getSentencasTexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iR9Oc6Yf6zMa"
      },
      "outputs": [],
      "source": [
        "def getSentencasTexto(textos, nlp = nlp):\n",
        "\n",
        "  \"\"\"\n",
        "     Sentencia um texto ou uma lista de textos.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "      `nlp` - Modelo spacy carregado.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista dos tokens\n",
        "  lista_sentencas = []\n",
        "\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    # Sentencia o documento\n",
        "    doc = nlp(texto)\n",
        "    sentencas = []\n",
        "\n",
        "    # Percorre as sentenças do documento\n",
        "    for sentenca in doc.sents:   \n",
        "      sentencas.append(str(sentenca))\n",
        "\n",
        "    lista_sentencas = lista_sentencas + sentencas  \n",
        "\n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_sentencas[0]\n",
        "  else:\n",
        "    return lista_sentencas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5czwzaxKza0y"
      },
      "source": [
        "## getSentencasMinusculo\n",
        "\n",
        "Retorna a lista das sentencas do texto em minúsculo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MQQAO4Raza0z"
      },
      "outputs": [],
      "source": [
        "def getSentencasMinusculo(textos):\n",
        "\n",
        "  \"\"\"\n",
        "     Sentencia um texto ou uma lista de textos em minusculo.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista dos tokens\n",
        "  lista_sentencas = []\n",
        "\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    lista_sentencas.append(str(texto).lower())\n",
        "      \n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_sentencas[0]\n",
        "  else:\n",
        "    return lista_sentencas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGaf7bkpAEiX"
      },
      "source": [
        "## getTokensTexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gWxyAo54AOHU"
      },
      "outputs": [],
      "source": [
        "def getTokensTexto(textos, nlp = nlp):\n",
        "\n",
        "  \"\"\"\n",
        "     Tokeniza um texto ou uma lista de textos.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista de retorno\n",
        "  lista_tokens_texto = []\n",
        "\n",
        "  # Percorre a lista de texto\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    # Verifica se o sentenca não foi processado pelo spaCy  \n",
        "    if type(texto) is not spacy.tokens.doc.Doc:\n",
        "        # Realiza o parsing no spacy\n",
        "        doc = nlp(texto)\n",
        "    else:\n",
        "        doc = texto\n",
        "\n",
        "    # Lista dos tokens\n",
        "    lista_tokens = []\n",
        "\n",
        "    # Percorre a sentença adicionando os tokens\n",
        "    for token in doc:    \n",
        "      lista_tokens.append(token.text)\n",
        "    \n",
        "    # Adiciona a lista de tokens na lista de sentenças\n",
        "    lista_tokens_texto.append(lista_tokens)\n",
        "\n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_tokens_texto[0]\n",
        "  else:\n",
        "    return lista_tokens_texto"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## removerPontuacao"
      ],
      "metadata": {
        "id": "l3VOqrF8h3-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removerPontuacao(textos):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        \n",
        "        doc = nlp(\" \".join(texto)) \n",
        "\n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ not in ['PUNCT']:\n",
        "              sentenca.append(token.text)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "R5P_9zfFh3-y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## relevantes"
      ],
      "metadata": {
        "id": "2C4s2rvzJ7iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relevantes(textos, postags_permitidas=['VER', 'AUX', 'NOUN']):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        \n",
        "        doc = nlp(\" \".join(texto)) \n",
        "      \n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ in postags_permitidas:\n",
        "              sentenca.append(token.text)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "5F6PEOkZJ7iv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lematizacao"
      ],
      "metadata": {
        "id": "1WOT9a_X5dkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizacao(textos, postags_permitidas=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        doc = nlp(\" \".join(texto)) \n",
        "\n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ in postags_permitidas:\n",
        "              sentenca.append(token.lemma_)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "SbnNOPv85d0C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preparaCorpus"
      ],
      "metadata": {
        "id": "b32wPnBG1faQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import das biblitecas\n",
        "import pandas as pd\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "def preparaCorpus(textos,                   \n",
        "                  sentenciaTexto=False,\n",
        "                  tornaMinusculo=False,\n",
        "                  removePontuacao=False, \n",
        "                  removeStopwords=False, \n",
        "                  bigramas=False, \n",
        "                  trigramas=False,\n",
        "                  somenteRelevante=False,\n",
        "                  postag_relevante=['VERB', 'AUX', 'NOUN'],\n",
        "                  lematizar=False,                  \n",
        "                  postag_lema=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \n",
        "    # Verifica se é um textos é str ou uma lista de texto\n",
        "    if type(textos) is str:\n",
        "      # Sentencia o texto\n",
        "      lista_sentencas = [textos]\n",
        "    else:\n",
        "      lista_sentencas = textos\n",
        "    \n",
        "    # Converte o texto em uma lista de sentencas\n",
        "    if sentenciaTexto==True:\n",
        "      lista_sentencas = getSentencasTexto(lista_sentencas)\n",
        "\n",
        "    # Converte o texto em minúsuclo\n",
        "    if tornaMinusculo==True:\n",
        "      lista_sentencas = getSentencasMinusculo(lista_sentencas)\n",
        "    \n",
        "    # tokeniza o texto\n",
        "    lista_sentencas_palavras = getTokensTexto(lista_sentencas)\n",
        "\n",
        "    # Remove a pontuação \n",
        "    if removePontuacao==True:\n",
        "        lista_sentencas_palavras = removerPontuacao(lista_sentencas_palavras)        \n",
        "\n",
        "    # Remove as stop words\n",
        "    if removeStopwords==True:\n",
        "      lista_sentencas_palavras = getTokensSemStopword(lista_sentencas_palavras)\n",
        "\n",
        "    # Criar bigramas ou trigramas\n",
        "    if bigramas==True:\n",
        "      # Construa os modelos de bigramas\n",
        "      bigram = gensim.models.Phrases(lista_sentencas_palavras, min_count=5, threshold=100) # max_topicse mais alto menos frases.\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "      bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "      lista_sentencas_palavras = [bigram_mod[doc] for doc in lista_sentencas_palavras]\n",
        "    \n",
        "    if trigramas==True:      \n",
        "      # Construa os modelos de bigramas\n",
        "      bigram = gensim.models.Phrases(lista_sentencas_palavras, min_count=5, threshold=100) # max_topicse mais alto menos frases.\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "      bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "      # Construa os modelos de trigramas\n",
        "      trigram = gensim.models.Phrases(bigram[lista_sentencas_palavras], threshold=100)\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama    \n",
        "      trigram_mod = gensim.models.phrases.Phraser(trigram)   \n",
        "      lista_sentencas_palavras = [trigram_mod[bigram_mod[doc]] for doc in lista_sentencas_palavras]   \n",
        "    \n",
        "    # Somente palavras relevantes\n",
        "    if somenteRelevante==True:      \n",
        "      lista_sentencas_palavras = relevantes(lista_sentencas_palavras, postags_permitidas=postag_relevante)\n",
        "    \n",
        "    # Faça a lematização mantendo apenas para noun, adj, vb, adv\n",
        "    if lematizar==True:      \n",
        "      lista_sentencas_palavras = lematizacao(lista_sentencas_palavras, postags_permitidas=postag_lema)\n",
        "\n",
        "    return lista_sentencas_palavras"
      ],
      "metadata": {
        "id": "rSW4ign41h1L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPuCCLyuBIeZ"
      },
      "source": [
        "# Exemplos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Conjunto de dados\n"
      ],
      "metadata": {
        "id": "BZQFeIobYaYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9QEWUZkfiiwd"
      },
      "outputs": [],
      "source": [
        "CohQuAD_Coh = [\n",
        "# 20 Perguntas do CohQuAD Coerentes\n",
        "\"Como enfileirar elementos em uma fila?\",      \n",
        "\"Como desenfileirar elementos em uma fila?\",\n",
        "\"Como empilhar elementos em uma pilha?\",\n",
        "\"Como empilhar e desempilhar elementos em uma pilha?\",\n",
        "\"Como empilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"Como empilhar e desempilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"Como desempilhar elementos em uma pilha?\",\n",
        "\"Como desempilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"O que é uma pilha e como empilhar seu elemento?\",\n",
        "\"O que é uma fila e como enfileirar seu elemento?\",\n",
        "\"O que é uma fila e como desenfileirar um elemento nela?\",\n",
        "\"O que é uma pilha e como desempilhar um elemento nela?\",\n",
        "\"O que é uma fila e como enfileirar um elemento nela?\",\n",
        "\"O que é uma pilha e como empilhar um elemento nela?\",\n",
        "\"O que é uma pilha e como empilhar e desempilhar seus elementos?\",\n",
        "\"O que é uma fila e como enfileirar e desenfileirar seus elementos?\",\n",
        "\"Como são implementadas as operações de empilhar e desempilhar elementos em uma pilha?\",\n",
        "\"Como são implementadas as operações de enfileirar e desenfileirar elementos em uma fila?\",\n",
        "\"Em uma pilha a operação de empilhar ocorre em qual extremidade?\",\n",
        "\"Em uma fila a operação de enfileirar ocorre em qual extremidade?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EwF293tUiiwi"
      },
      "outputs": [],
      "source": [
        "CohQuAD_Inc = [\n",
        "# 20 Perguntas do CohQuAD Incoerentes\n",
        "\"Como enfileirar elementos em uma pilha?\",\n",
        "\"Como desenfileirar elementos em uma pilha?\",\n",
        "\"Como empilhar elementos em uma fila?\",\n",
        "\"Como empilhar e desempilhar elementos em uma fila?\",\n",
        "\"Como empilhar elementos em uma estrutura de dados fila?\",\n",
        "\"Como empilhar e desempilhar elementos em uma estrutura de dados fila?\",\n",
        "\"Como desempilhar elementos em uma fila?\",\n",
        "\"Como desempilhar elementos em uma estrutura de dados fila?\",\n",
        "\"O que é uma fila e como empilhar seu elemento?\",\n",
        "\"O que é uma pilha e como enfileirar seu elemento?\",\n",
        "\"O que é uma pilha e como desenfileirar um elemento nela?\",\n",
        "\"O que é uma fila e como desempilhar um elemento nela?\",\n",
        "\"O que é uma pilha e como enfileirar um elemento nela?\",\n",
        "\"O que é uma fila e como empilhar um elemento nela?\",\n",
        "\"O que é uma fila e como empilhar e desempilhar seus elementos?\",\n",
        "\"O que é uma pilha e como enfileirar e desenfileirar seus elementos?\",\n",
        "\"Como são implementadas as operações de empilhar e desempilhar elementos em uma fila?\",\n",
        "\"Como são implementadas as operações de enfileirar e desenfileirar elementos em uma pilha?\",\n",
        "\"Em uma pilha a operação de enfileirar ocorre em qual extremidade?\",\n",
        "\"Em uma fila a operação de empilhar ocorre em qual extremidade?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando o dicionário sem lematização e sem as stopwords\n",
        "\n",
        "textos = preparaCorpus(CohQuAD_Coh,                        \n",
        "                       sentenciaTexto=True,\n",
        "                       tornaMinusculo=True,\n",
        "                       removePontuacao=True,\n",
        "                       somenteRelevante=False,\n",
        "                       removeStopwords=False)"
      ],
      "metadata": {
        "id": "aJecgsByjHDu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Cálculo de PMI"
      ],
      "metadata": {
        "id": "U02pkc1ueo60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 1"
      ],
      "metadata": {
        "id": "xQGqu8jXjXe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens = len(textos[0])\n",
        "print(n_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxnoFaT_rhKH",
        "outputId": "87bcb542-23fc-46ab-ede0-acf98283a0e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sem janela\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela\n",
        "def gerarBigramasJanela(data, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(data)):\n",
        "        janela = data[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama"
      ],
      "metadata": {
        "id": "a-246Qn3kc-5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "\n",
        "textos_bgs = gerarBigramasJanela(documento)\n",
        "print(textos_bgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg2W1Eagks2m",
        "outputId": "d1552052-0f69-477e-de8f-fbba0ff84c5e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['como', 'enfileirar'], ['enfileirar', 'elementos'], ['elementos', 'em'], ['em', 'uma'], ['uma', 'fila']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contarFrequenciaPalavra(palavra, doc):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq"
      ],
      "metadata": {
        "id": "RJ88iAagqX_X"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_w1 = contarFrequenciaPalavra(doc=textos[0],palavra='como')\n",
        "print(n_w1)\n",
        "n_w2 = contarFrequenciaPalavra(doc=textos[0],palavra='enfileirar')\n",
        "print(n_w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcTYoAI2qdoe",
        "outputId": "024f6a5f-4036-489b-d963-c7609cd6d217"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "        if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "            freq = freq + 1\n",
        "    return freq"
      ],
      "metadata": {
        "id": "-meBRm9CsUvt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_w1_w2 = contaFrequenciaBigrama(w1='como', w2='enfileirar',bigrams=textos_bgs)\n",
        "print(n_w1_w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8HSK08_j0Nf",
        "outputId": "b7d36efe-0a3b-45bb-abbd-dd62af8d39d0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "epsilon = 1\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def pmi(P_x, P_y, P_xy):\n",
        "    return math.log((P_xy+epsilon) / (P_x * P_y))\n",
        "\n",
        "p_w1 = probabilidade(n_w1, n_tokens)\n",
        "\n",
        "p_w2 = probabilidade(n_w2, n_tokens)\n",
        "\n",
        "p_w1_w2 = probabilidade(n_w1_w2, len(textos_bgs))\n",
        "\n",
        "r = pmi(p_w1, p_w2, p_w1_w2)\n",
        "\n",
        "print(\"pmi para como e enfileirar = \",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLNHw5Zgl433",
        "outputId": "c50098bc-a616-4469-a337-029e0fab0264"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pmi para como e enfileirar =  3.765840495250065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 2"
      ],
      "metadata": {
        "id": "gze2XHNx3S4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob_ngram_a = 0\n",
        "\n",
        "prob_ngram_a = 1 if (prob_ngram_a == 0) else prob_ngram_a\n",
        "\n",
        "print(prob_ngram_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1E77c6gd_oj",
        "outputId": "1b80ab34-7937-4d99-939a-d237938bf48f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Sem janela variável\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela variável\n",
        "def gerarBigramasJanela(doc, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(doc)):\n",
        "        janela = doc[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama\n",
        "\n",
        "def contarFrequenciaPalavra(doc, palavra):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq\n",
        "\n",
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "      if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "        freq = freq + 1\n",
        "    return freq\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def calculaPMI(w1, w2, doc, tamanho_janela=2):\n",
        "    \n",
        "    # Quantidade tokens do texto\n",
        "    n_tokens = len(doc)\n",
        "    # print(\"n_tokens:\",n_tokens)\n",
        "    \n",
        "    # Conta as frequencias de w1 e w2\n",
        "    n_w1 = contarFrequenciaPalavra(doc,w1)\n",
        "    # print(\"n_w1:\",n_w1)\n",
        "    n_w2 = contarFrequenciaPalavra(doc,w2)    \n",
        "    # print(\"n_w2:\",n_w2)\n",
        "\n",
        "    # Gera os bigrammas do documento\n",
        "    # bgs = gerarBigramas(doc)\n",
        "    bgs = gerarBigramasJanela(doc,tamanho_janela=tamanho_janela)\n",
        "\n",
        "    # Conta a frequencia no brigrama\n",
        "    n_w1_w2 = contaFrequenciaBigrama(w1, w2 , bigrams=bgs)\n",
        "    # print(\"n_w1_w2:\",n_w1_w2)\n",
        "    \n",
        "    # Calcula as probabilidades\n",
        "    p_w1 = probabilidade(n_w1, n_tokens)\n",
        "    # print(\"p_w1:\",p_w1)\n",
        "\n",
        "    p_w2 = probabilidade(n_w2, n_tokens)\n",
        "    # print(\"p_w2:\",p_w2)\n",
        "\n",
        "    p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    # print(\"p_w1_w2:\",p_w1_w2)\n",
        "\n",
        "    # Para evitar divisão por 0 e log de 0\n",
        "    # epsilon = 1\n",
        "    epsilon = sys.float_info.min\n",
        "\n",
        "    produto = (p_w1_w2+epsilon) / ((p_w1 * p_w2)+epsilon)\n",
        "    if produto == 0:\n",
        "      produto = 1\n",
        "\n",
        "    r = math.log(produto)\n",
        "    \n",
        "    return r"
      ],
      "metadata": {
        "id": "sDsQZQ4pyviQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo pmi"
      ],
      "metadata": {
        "id": "_1dXCwz7zQo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "\n",
        "w1 = 'como'\n",
        "w2 = 'enfileirar'\n",
        "r = calculaPMI(w1, w2, documento)\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxFdneuCz-UK",
        "outputId": "78b834c7-36c1-4f29-d5ba-195f45b65bb1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pmi para 'como' e 'enfileirar' = 1.791759469228055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simetria"
      ],
      "metadata": {
        "id": "1Tg-lCbzzS-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "\n",
        "w1 = 'enfileirar'\n",
        "w2 = 'como'\n",
        "r = calculaPMI(w1, w2, documento)\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF0Ostl9DjSG",
        "outputId": "ae854dc2-eca8-474b-fec3-3125d67f4cfb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pmi para 'enfileirar' e 'como' = 1.791759469228055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo igual a\n",
        "https://www.listendata.com/2022/06/pointwise-mutual-information-pmi.html"
      ],
      "metadata": {
        "id": "iSII2aSvzWZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento = ['this','is','a','foo','bar','bar','black','sheep','foo','bar','bar','black','sheep','foo','bar','bar','black','sheep','shep','bar','bar','black','sentence']\n",
        "print(documento)\n",
        "\n",
        "w1 = 'foo'\n",
        "w2 = 'bar'\n",
        "\n",
        "r = calculaPMI(w1, w2, documento)\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB878l99gfnx",
        "outputId": "c086435c-22fc-4a9d-8df8-e72b25e3aae1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'foo', 'bar', 'bar', 'black', 'sheep', 'foo', 'bar', 'bar', 'black', 'sheep', 'foo', 'bar', 'bar', 'black', 'sheep', 'shep', 'bar', 'bar', 'black', 'sentence']\n",
            "pmi para 'foo' e 'bar' = 1.0560526742493137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documento = ['I','creating','code','python','calculates','point','mutual','information','document','python','code','coming','along','nicely']\n",
        "print(documento)\n",
        "\n",
        "w1 = 'python'\n",
        "w2 = 'code'\n",
        "\n",
        "r = calculaPMI(w1, w2, documento, tamanho_janela=5)\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YefAw3JQ89wj",
        "outputId": "0682b9a2-b2e9-414f-cb15-ab11f132a430"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'creating', 'code', 'python', 'calculates', 'point', 'mutual', 'information', 'document', 'python', 'code', 'coming', 'along', 'nicely']\n",
            "pmi para 'python' e 'code' = 1.9459101490553132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Cálculo de NPMI\n",
        "\n",
        "Intervalo [-1 , 1]"
      ],
      "metadata": {
        "id": "iMp8LD_v6jvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 1"
      ],
      "metadata": {
        "id": "A-21Uckv6jvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela\n",
        "def gerarBigramasJanela(doc, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(doc)):\n",
        "        janela = doc[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama\n",
        "\n",
        "def contarFrequenciaPalavra(doc, palavra):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq\n",
        "\n",
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "        if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "        # if (bg[0] == w1 and bg[1] == w2):\n",
        "          freq = freq + 1\n",
        "    return freq\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def calculaNPMI(w1, w2, doc, tamanho_janela=2):\n",
        "    \n",
        "    # Quantidade tokens do texto\n",
        "    n_tokens = len(doc)\n",
        "    # print(\"n_tokens:\",n_tokens)\n",
        "\n",
        "    # Gera os bigrammas do documento    \n",
        "    bgs = gerarBigramasJanela(doc,tamanho_janela)\n",
        "\n",
        "    # Conta a frequencia no brigrama\n",
        "    n_w1_w2 = contaFrequenciaBigrama(w1, w2 , bigrams=bgs)\n",
        "    # print(\"n_w1_w2:\",n_w1_w2)\n",
        "\n",
        "    # Calcula as probabilidades        \n",
        "    p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    # print(\"p_w1_w2:\",p_w1_w2)\n",
        "    \n",
        "    # Para evitar divisão por 0 e log de 0\n",
        "    e = sys.float_info.min\n",
        "    \n",
        "    r = calculaPMI(w1, w2, doc, tamanho_janela=2)\n",
        "\n",
        "    nr = r / -(math.log(p_w1_w2 + e))\n",
        "    return nr"
      ],
      "metadata": {
        "id": "6_r45HT36jvW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "\n",
        "w1 = 'como'\n",
        "w2 = 'enfileirar'\n",
        "r = calculaNPMI(w1, w2, documento)\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca47e8b5-6d7c-4e1c-f4a0-f3c7ad46db0c",
        "id": "uJtR9-vi6jvX"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npmi para 'como' e 'enfileirar' = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "simetria"
      ],
      "metadata": {
        "id": "siVQBNikpH9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "print(\"documento:\",documento)\n",
        "\n",
        "w1 = 'enfileirar'\n",
        "w2 = 'como'\n",
        "r = calculaNPMI(w2, w1, documento)\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWoueZ56DcSo",
        "outputId": "837bea49-e2d7-4064-be87-2ac9267450e1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documento: ['como', 'enfileirar', 'elementos', 'em', 'uma', 'fila']\n",
            "npmi para 'enfileirar' e 'como' = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "print(\"documento:\",documento)\n",
        "\n",
        "w1 = 'como'\n",
        "w2 = 'elementos'\n",
        "r = calculaNPMI(w1, w2, documento)\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5w4J76aST4f",
        "outputId": "7b097bad-7d5b-4ddc-f1e6-4a832993aefb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documento: ['como', 'enfileirar', 'elementos', 'em', 'uma', 'fila']\n",
            "npmi para 'como' e 'elementos' = -0.9949413649692346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Palavras distantes"
      ],
      "metadata": {
        "id": "lcmVmqXOpSuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "print(\"documento:\",documento)\n",
        "\n",
        "w1 = 'como'\n",
        "w2 = 'fila'\n",
        "r = calculaNPMI(w1, w2, documento)\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM1E5zPvpGob",
        "outputId": "c47471de-56af-474b-e9da-4c829cffb357"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documento: ['como', 'enfileirar', 'elementos', 'em', 'uma', 'fila']\n",
            "npmi para 'como' e 'fila' = -0.9949413649692346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Palavras inexistentes"
      ],
      "metadata": {
        "id": "I4pOPXzYpUKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "print(\"documento:\",documento)\n",
        "\n",
        "w1 = 'como'\n",
        "w2 = 'pilha'\n",
        "r = calculaNPMI(w1, w2, documento)\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOEUQfA6pW3-",
        "outputId": "a58ca3dc-64b5-4f47-d2d3-ee4a59f86bb9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documento: ['como', 'enfileirar', 'elementos', 'em', 'uma', 'fila']\n",
            "npmi para 'como' e 'pilha' = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Cálculo de $C_{UCI}$"
      ],
      "metadata": {
        "id": "k8sVosaZ2AZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculaCUCI(topicos, documento):\n",
        "\n",
        "  # Quantidade tópicos  \n",
        "  K = len(topicos)\n",
        "  # print(\"K:\",K)\n",
        "\n",
        "  somaUCITopico = 0\n",
        "  for k in range(K):\n",
        "\n",
        "    N = len(topicos[k])\n",
        "    if N < 2:\n",
        "      print(\"Deve existir pelo menos 2 tópicos!\")\n",
        "\n",
        "    somaUCI = 0\n",
        "    for i in range(0,N-1):\n",
        "      for j in range(i+1,N):\n",
        "        # Recupera o 1o tópico\n",
        "        wi = topicos[k][i]\n",
        "        # print(\"wi:\",wi)\n",
        "        # Recupera o 2o tópico\n",
        "        wj = topicos[k][j]\n",
        "        # print(\"wj:\",wj)\n",
        "\n",
        "        # Calcula PMI\n",
        "        pmi = calculaPMI(wi, wj, documento, tamanho_janela=10)        \n",
        "        # print(\"pmi:\", pmi)\n",
        "\n",
        "        # Acumula o pmi\n",
        "        somaUCI = somaUCI + pmi\n",
        "    \n",
        "    mediaUCI = somaUCI/N\n",
        "\n",
        "    somaUCITopico = somaUCITopico + mediaUCI\n",
        "    \n",
        "  # print(\"Soma UCI:\", somaUCI)\n",
        "  cuci = somaUCITopico/K\n",
        "  # print(\"cuci:\", cuci)\n",
        "  \n",
        "  return cuci"
      ],
      "metadata": {
        "id": "jt9SZ7nl2Gbw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documento = textos[0]\n",
        "print(\"Documento:\", documento)\n",
        "\n",
        "#Lista de pares de tópicos\n",
        "topicos = [['como','enfileirar']]\n",
        "print(\"Tópicos:\", topicos)\n",
        "\n",
        "cuci = calculaCUCI(topicos, documento)\n",
        "\n",
        "print(\"C_UCI:\", cuci)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN-aqhKmqTZ8",
        "outputId": "98f0abcc-51da-4de3-b58c-afe7c70a38d3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento: ['como', 'enfileirar', 'elementos', 'em', 'uma', 'fila']\n",
            "Tópicos: [['como', 'enfileirar']]\n",
            "C_UCI: 0.8958797346140275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Cálculo de $C_{NPMI}$\n",
        "\n",
        "https://github.com/suzyahyah/topic_coherence/blob/master/npmi.py"
      ],
      "metadata": {
        "id": "fACw2r1s5gP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculaCNPMI(topicos, texto):\n",
        "\n",
        "  # Quantidade tópicos\n",
        "  K = len(topicos)\n",
        "  # print(\"K:\",K)\n",
        "\n",
        "  somaNPMITopico = 0\n",
        "\n",
        "  for k in range(K):\n",
        "\n",
        "    N = len(topicos[k])\n",
        "    if N < 2:\n",
        "      print(\"Deve existir pelo menos 2 tópicos!\")\n",
        "\n",
        "    somaNPMI = 0\n",
        "    for i in range(0,N-1):\n",
        "      for j in range(i+1,N):   \n",
        "        # Recupera o 1o tópico   \n",
        "        wi = topicos[k][i]\n",
        "        # print(\"wi:\",wi)\n",
        "        # Recupera o 1o tópico\n",
        "        wj = topicos[k][j]\n",
        "        # print(\"wj:\",wj)\n",
        "\n",
        "        # Calcula PMI\n",
        "        npmi = calculaNPMI(wi, wj, texto, tamanho_janela=10)        \n",
        "        # print(\"npmi:\", npmi)\n",
        "\n",
        "        # Acumula o npmi\n",
        "        somaNPMI = somaNPMI + npmi\n",
        "    \n",
        "    mediaNPMI = somaNPMI/N\n",
        "\n",
        "    somaNPMITopico = somaNPMITopico + mediaNPMI\n",
        "    \n",
        "  # print(\"Soma NPMI:\", somaNPMI)\n",
        "  cnpmi = somaNPMITopico/K\n",
        "  # print(\"cnpmi:\", cnpmi)\n",
        "  \n",
        "  return cnpmi"
      ],
      "metadata": {
        "id": "Y2JnfSX2h7Gc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = textos[0]\n",
        "print(\"Texto:\", texto)\n",
        "\n",
        "#Lista de pares de tópicos\n",
        "topicos = [['como','enfileirar']]\n",
        "print(\"Tópicos:\", topicos)\n",
        "\n",
        "cnpmi = calculaCNPMI(topicos, texto)\n",
        "\n",
        "print(\"C_NPMI:\", cnpmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00543ec3-b1ae-400a-8007-e092919334c3",
        "id": "kuXRMg945gP2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: ['como', 'enfileirar', 'elementos', 'em', 'uma', 'fila']\n",
            "Tópicos: [['como', 'enfileirar']]\n",
            "C_NPMI: 0.5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+BpKgecsHKYhcDZnkKMvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}