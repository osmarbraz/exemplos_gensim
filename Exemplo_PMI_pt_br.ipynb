{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_gensim/blob/master/Exemplo_PMI_pt_br.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IziPWVaeGlWk"
      },
      "source": [
        "# Exemplo PMI em pt-br\n",
        "\n",
        "https://python.plainenglish.io/collocation-discovery-with-pmi-3bde8f351833\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "##Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "outputs": [],
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "outputs": [],
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RufkKnojlwzu"
      },
      "source": [
        "# 1 - Instalação do spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LeiOTx0Dlk"
      },
      "source": [
        "https://spacy.io/\n",
        "\n",
        "Modelos do spaCy para português:\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "pYSkCUy-Dsdy",
        "outputId": "cf17efad-fe8b-4cfb-b79e-f8f2cdfcd471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 32.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.38.1)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.38.1\n",
            "    Uninstalling wheel-0.38.1:\n",
            "      Successfully uninstalled wheel-0.38.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "Successfully installed pip-22.3.1 setuptools-65.5.1 wheel-0.38.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Instala o spacy\n",
        "!pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Fvx0TVRQUw",
        "outputId": "694f105f-dbb2-4579-941e-90578e5466b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==3.2.0\n",
            "  Downloading spacy-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.7.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.10)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m660.6/660.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.4.2)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (65.5.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (4.64.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2022.9.24)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.0) (2.0.1)\n",
            "Installing collected packages: typing-extensions, pydantic, thinc, spacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.1.1\n",
            "    Uninstalling typing_extensions-4.1.1:\n",
            "      Successfully uninstalled typing_extensions-4.1.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.2\n",
            "    Uninstalling spacy-3.4.2:\n",
            "      Successfully uninstalled spacy-3.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.8.2 spacy-3.2.0 thinc-8.0.17 typing-extensions-3.10.0.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Instala uma versão específica\n",
        "!pip install -U spacy==3.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GwcgkOlWi3"
      },
      "source": [
        "Realiza o download e carrega os modelos necessários a biblioteca\n",
        "\n",
        "https://spacy.io/models/pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z4LqE5kTwDYm"
      },
      "outputs": [],
      "source": [
        "# Definição do nome do arquivo do modelo\n",
        "#ARQUIVOMODELO = \"pt_core_news_sm\"\n",
        "#ARQUIVOMODELO = \"pt_core_news_md\"\n",
        "ARQUIVOMODELO = \"pt_core_news_lg\"\n",
        "\n",
        "# Definição da versão da spaCy\n",
        "VERSAOSPACY = \"-3.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aJ2KB3UCp-ws"
      },
      "outputs": [],
      "source": [
        "#Baixa automaticamente o arquivo do modelo.\n",
        "#!python -m spacy download {ARQUIVOMODELO}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASk5iFeUp9LE",
        "outputId": "d06ca298-9ac0-4b5b-a7fd-5567ad4087fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-09 23:58:28--  https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.2.0/pt_core_news_lg-3.2.0.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/fcaf57f0-07de-4dbc-9419-3b54eb2651b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221109%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221109T235828Z&X-Amz-Expires=300&X-Amz-Signature=7b071662374e0e535d513521531ce88c101119fa68b929abd138024ee8b59cd5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-3.2.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-11-09 23:58:28--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/fcaf57f0-07de-4dbc-9419-3b54eb2651b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221109%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221109T235828Z&X-Amz-Expires=300&X-Amz-Signature=7b071662374e0e535d513521531ce88c101119fa68b929abd138024ee8b59cd5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Dpt_core_news_lg-3.2.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577393393 (551M) [application/octet-stream]\n",
            "Saving to: ‘pt_core_news_lg-3.2.0.tar.gz’\n",
            "\n",
            "pt_core_news_lg-3.2 100%[===================>] 550.64M  12.6MB/s    in 14s     \n",
            "\n",
            "2022-11-09 23:58:42 (40.6 MB/s) - ‘pt_core_news_lg-3.2.0.tar.gz’ saved [577393393/577393393]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Realiza o download do arquivo do modelo para o diretório corrente\n",
        "!wget https://github.com/explosion/spacy-models/releases/download/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_LkF7Nfm8_"
      },
      "source": [
        "Descompacta o arquivo do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9fCQQJGeVEY",
        "outputId": "be1e913d-3038-4f4c-daed-d09b0d25b67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_core_news_lg-3.2.0/\n",
            "pt_core_news_lg-3.2.0/LICENSE\n",
            "pt_core_news_lg-3.2.0/LICENSES_SOURCES\n",
            "pt_core_news_lg-3.2.0/MANIFEST.in\n",
            "pt_core_news_lg-3.2.0/PKG-INFO\n",
            "pt_core_news_lg-3.2.0/README.md\n",
            "pt_core_news_lg-3.2.0/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/__init__.py\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/LICENSE\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/LICENSES_SOURCES\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/README.md\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/accuracy.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/attribute_ruler/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/attribute_ruler/patterns\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/config.cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/lookups/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/lemmatizer/lookups/lookups.bin\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/meta.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/morphologizer/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/ner/moves\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/parser/moves\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/senter/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tok2vec/model\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/tokenizer\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/key2row\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/lookups.bin\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/strings.json\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/vectors\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg/pt_core_news_lg-3.2.0/vocab/vectors.cfg\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/PKG-INFO\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/SOURCES.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/dependency_links.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/entry_points.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/not-zip-safe\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/requires.txt\n",
            "pt_core_news_lg-3.2.0/pt_core_news_lg.egg-info/top_level.txt\n",
            "pt_core_news_lg-3.2.0/setup.cfg\n",
            "pt_core_news_lg-3.2.0/setup.py\n"
          ]
        }
      ],
      "source": [
        "# Descompacta o arquivo do modelo\n",
        "!tar -xvf  /content/{ARQUIVOMODELO}{VERSAOSPACY}.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ovOx-3Wb-JJW"
      },
      "outputs": [],
      "source": [
        "# Coloca a pasta do modelo descompactado em uma pasta de nome mais simples\n",
        "!mv /content/{ARQUIVOMODELO}{VERSAOSPACY}/{ARQUIVOMODELO}/{ARQUIVOMODELO}{VERSAOSPACY} /content/{ARQUIVOMODELO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHT2c89qvwK"
      },
      "source": [
        "Carrega o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nbELnrpgA4T1"
      },
      "outputs": [],
      "source": [
        "# Import das bibliotecas.\n",
        "import spacy\n",
        "\n",
        "CAMINHOMODELO = \"/content/\" + ARQUIVOMODELO\n",
        "\n",
        "nlp = spacy.load(CAMINHOMODELO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTTdqxKQ1Ay"
      },
      "source": [
        "Recupera os stopwords do spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OBInu7ayQ31J"
      },
      "outputs": [],
      "source": [
        "# Recupera as stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_EYNu-_RX7k"
      },
      "source": [
        "Lista dos stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUSaUJEWRbnZ",
        "outputId": "114bf69f-6b07-4ad6-ab9c-746c33eb2109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de stopwords: 416\n",
            "{'quê', 'uns', 'nesse', 'teu', 'dezoito', 'grande', 'pelos', 'vinda', 'querem', 'nosso', 'mesmo', 'qualquer', 'dar', 'pontos', 'próximo', 'fim', 'esses', 'tens', 'forma', 'somos', 'os', 'oitavo', 'tempo', 'apenas', 'vezes', 'fui', 'com', 'este', 'o', 'exemplo', 'terceira', 'pois', 'esteve', 'estar', 'dentro', 'uma', 'só', 'caminho', 'pouco', 'veja', 'tendes', 'para', 'segundo', 'primeiro', 'esta', 'as', 'ter', 'grandes', 'debaixo', 'tem', 'cujo', 'qual', 'esse', 'põem', 'ademais', 'partir', 'usa', 'porquanto', 'onde', 'estará', 'fez', 'área', 'pela', 'obrigada', 'porquê', 'lá', 'atrás', 'assim', 'estado', 'final', 'nove', 'aquela', 'números', 'novas', 'também', 'estes', 'longe', 'apontar', 'embora', 'é', 'estiveste', 'ver', 'zero', 'acerca', 'pegar', 'dizer', 'tentar', 'quer', 'apoio', 'por', 'conselho', 'neste', 'oito', 'vosso', 'às', 'do', 'dessa', 'certeza', 'sou', 'iniciar', 'contudo', 'eu', 'ligado', 'após', 'podia', 'sois', 'maior', 'tudo', 'és', 'bem', 'vais', 'meio', 'da', 'ali', 'cento', 'vêm', 'tão', 'mil', 'demais', 'grupo', 'tanta', 'tanto', 'estou', 'adeus', 'todas', 'algumas', 'dezassete', 'vindo', 'até', 'três', 'seis', 'pelo', 'vão', 'sempre', 'estava', 'coisa', 'favor', 'meus', 'todos', 'vós', 'desta', 'muito', 'bastante', 'valor', 'desse', 'foram', 'pôde', 'estiveram', 'num', 'vossa', 'foste', 'saber', 'dezasseis', 'inclusive', 'nuns', 'número', 'dezanove', 'seria', 'umas', 'des', 'tivestes', 'meu', 'quarto', 'estão', 'podem', 'faz', 'aqui', 'se', 'algo', 'bom', 'vocês', 'vai', 'fostes', 'naquele', 'nas', 'boa', 'tenho', 'ao', 'fazeis', 'seus', 'então', 'maioria', 'lado', 'próprio', 'vossos', 'alguns', 'está', 'são', 'sabe', 'vários', 'estivestes', 'logo', 'portanto', 'local', 'têm', 'quarta', 'ainda', 'menor', 'não', 'novo', 'fazer', 'seu', 'cá', 'máximo', 'desde', 'parte', 'primeira', 'temos', 'sexto', 'quieta', 'nunca', 'último', 'me', 'todo', 'fora', 'põe', 'um', 'naquela', 'maiorias', 'povo', 'você', 'ambos', 'certamente', 'como', 'deste', 'nos', 'treze', 'sobre', 'quais', 'quatro', 'comprido', 'terceiro', 'ou', 'possivelmente', 'fazes', 'direita', 'sua', 'somente', 'te', 'tentaram', 'cinco', 'talvez', 'quando', 'aí', 'cada', 'isto', 'cuja', 'que', 'teve', 'catorze', 'posso', 'pelas', 'minha', 'pouca', 'fomos', 'cima', 'através', 'devem', 'tente', 'dão', 'numa', 'suas', 'sem', 'estivemos', 'tais', 'era', 'tiveste', 'mês', 'nível', 'além', 'obrigado', 'e', 'faço', 'próxima', 'tiveram', 'eventual', 'mal', 'cedo', 'inicio', 'porém', 'sétima', 'deverá', 'aos', 'essa', 'contra', 'irá', 'tentei', 'parece', 'estás', 'no', 'das', 'depois', 'quem', 'toda', 'aqueles', 'minhas', 'ser', 'enquanto', 'nossas', 'daquela', 'sexta', 'estive', 'quero', 'já', 'deve', 'dos', 'quinze', 'sei', 'geral', 'mas', 'aquilo', 'outros', 'diz', 'tuas', 'meses', 'momento', 'fazia', 'outras', 'muitos', 'menos', 'sob', 'vos', 'nesta', 'falta', 'teus', 'ambas', 'tua', 'antes', 'eles', 'vens', 'tu', 'nenhuma', 'agora', 'nossa', 'entre', 'ora', 'dois', 'de', 'breve', 'doze', 'disso', 'aquele', 'possível', 'quinto', 'onze', 'lugar', 'fazemos', 'sistema', 'corrente', 'a', 'fará', 'nós', 'oitava', 'vossas', 'tarde', 'quanto', 'outra', 'novos', 'diante', 'isso', 'quieto', 'usar', 'sétimo', 'relação', 'tivemos', 'na', 'dez', 'dá', 'tal', 'nessa', 'nada', 'quinta', 'nossos', 'fazem', 'conhecida', 'apoia', 'porque', 'duas', 'ir', 'tive', 'ontem', 'questão', 'essas', 'tipo', 'conhecido', 'vez', 'sete', 'puderam', 'estas', 'pode', 'for', 'elas', 'custa', 'sim', 'à', 'perto', 'vinte', 'comprida', 'dizem', 'lhe', 'segunda', 'posição', 'poder', 'daquele', 'ponto', 'ele', 'mais', 'nova', 'poderá', 'em', 'foi', 'aquelas', 'nem', 'vem', 'ela', 'baixo'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Quantidade de stopwords:\", len(spacy_stopwords))\n",
        "\n",
        "print(spacy_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyEaXKeaLWlq"
      },
      "source": [
        "## getTokensSemStopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pbUf_V_1axS2"
      },
      "outputs": [],
      "source": [
        "def getTokensSemStopword(tokens, spacy_stopwords=spacy_stopwords):\n",
        "    \"\"\"\n",
        "      Retira os tokens da lista de tokens tokens que estão na lista de stopword.\n",
        "      A lista de tokens pode ou não estar dentro de uma outra lista.\n",
        "    \n",
        "      Parâmetros:\n",
        "        `tokens` - Uma lista com os tokens ou uma lista de lista de tokens.\n",
        "        `spacy_stopwords` - Uma lista com as stopword. \n",
        "    \"\"\"\n",
        "    \n",
        "    # Verifica se é uma lista de palavras(str) ou ou uma lista de lista\n",
        "    if type(tokens[0]) is str:\n",
        "      lista_tokens = [tokens]\n",
        "    else:\n",
        "      lista_tokens = tokens\n",
        "      \n",
        "    # Lista de retorno\n",
        "    lista_tokens_sem_stopwords = []  \n",
        "\n",
        "    # Percorre a lista de tokens\n",
        "    for texto in lista_tokens:\n",
        "\n",
        "      # Lista dos tokens sem as stopwords\n",
        "      tokens_sem_stopwords = []\n",
        "      \n",
        "      # Percorre os tokens    \n",
        "      for token in texto:\n",
        "        # Verifica se o toke não está na lista de stopwords para adicionar a nova lista\n",
        "        if token not in spacy_stopwords:\n",
        "          tokens_sem_stopwords.append(token)\n",
        "      \n",
        "      # Adiciona a lista de tokens sem stopwords na lista de retorno se tiver uma palavra\n",
        "      if len(tokens_sem_stopwords) != 0:\n",
        "        lista_tokens_sem_stopwords.append(tokens_sem_stopwords)\n",
        "\n",
        "    if type(tokens[0]) is str:      \n",
        "      return lista_tokens_sem_stopwords[0]\n",
        "    else:\n",
        "      return lista_tokens_sem_stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7XoLBuW6woe"
      },
      "source": [
        "## getSentencasTexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iR9Oc6Yf6zMa"
      },
      "outputs": [],
      "source": [
        "def getSentencasTexto(textos, nlp = nlp):\n",
        "\n",
        "  \"\"\"\n",
        "     Sentencia um texto ou uma lista de textos.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "      `nlp` - Modelo spacy carregado.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista dos tokens\n",
        "  lista_sentencas = []\n",
        "\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    # Sentencia o documento\n",
        "    doc = nlp(texto)\n",
        "    sentencas = []\n",
        "\n",
        "    # Percorre as sentenças do documento\n",
        "    for sentenca in doc.sents:   \n",
        "      sentencas.append(str(sentenca))\n",
        "\n",
        "    lista_sentencas = lista_sentencas + sentencas  \n",
        "\n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_sentencas[0]\n",
        "  else:\n",
        "    return lista_sentencas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5czwzaxKza0y"
      },
      "source": [
        "## getSentencasMinusculo\n",
        "\n",
        "Retorna a lista das sentencas do texto em minúsculo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MQQAO4Raza0z"
      },
      "outputs": [],
      "source": [
        "def getSentencasMinusculo(textos):\n",
        "\n",
        "  \"\"\"\n",
        "     Sentencia um texto ou uma lista de textos em minusculo.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista dos tokens\n",
        "  lista_sentencas = []\n",
        "\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    lista_sentencas.append(str(texto).lower())\n",
        "      \n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_sentencas[0]\n",
        "  else:\n",
        "    return lista_sentencas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGaf7bkpAEiX"
      },
      "source": [
        "## getTokensTexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gWxyAo54AOHU"
      },
      "outputs": [],
      "source": [
        "def getTokensTexto(textos, nlp = nlp):\n",
        "\n",
        "  \"\"\"\n",
        "     Tokeniza um texto ou uma lista de textos.\n",
        "    \n",
        "     Parâmetros:\n",
        "      `textos` - Um texto(str) ou uma lista de textos.\n",
        "  \"\"\"\n",
        "\n",
        "  # Verifica se é um texto é str ou uma lista de texto\n",
        "  if type(textos) is str:\n",
        "    lista_texto = [textos]\n",
        "  else:\n",
        "    lista_texto = textos\n",
        "\n",
        "  # Lista de retorno\n",
        "  lista_tokens_texto = []\n",
        "\n",
        "  # Percorre a lista de texto\n",
        "  for texto in lista_texto:\n",
        "\n",
        "    # Verifica se o sentenca não foi processado pelo spaCy  \n",
        "    if type(texto) is not spacy.tokens.doc.Doc:\n",
        "        # Realiza o parsing no spacy\n",
        "        doc = nlp(texto)\n",
        "    else:\n",
        "        doc = texto\n",
        "\n",
        "    # Lista dos tokens\n",
        "    lista_tokens = []\n",
        "\n",
        "    # Percorre a sentença adicionando os tokens\n",
        "    for token in doc:    \n",
        "      lista_tokens.append(token.text)\n",
        "    \n",
        "    # Adiciona a lista de tokens na lista de sentenças\n",
        "    lista_tokens_texto.append(lista_tokens)\n",
        "\n",
        "  # Verifica o tipo documento para o tipo de retorno\n",
        "  if type(textos) is str:\n",
        "    return lista_tokens_texto[0]\n",
        "  else:\n",
        "    return lista_tokens_texto"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## removerPontuacao"
      ],
      "metadata": {
        "id": "l3VOqrF8h3-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removerPontuacao(textos):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        \n",
        "        doc = nlp(\" \".join(texto)) \n",
        "\n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ not in ['PUNCT']:\n",
        "              sentenca.append(token.text)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "R5P_9zfFh3-y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## relevantes"
      ],
      "metadata": {
        "id": "2C4s2rvzJ7iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relevantes(textos, postags_permitidas=['VER', 'AUX', 'NOUN']):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        \n",
        "        doc = nlp(\" \".join(texto)) \n",
        "      \n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ in postags_permitidas:\n",
        "              sentenca.append(token.text)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "5F6PEOkZJ7iv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lematizacao"
      ],
      "metadata": {
        "id": "1WOT9a_X5dkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizacao(textos, postags_permitidas=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "\n",
        "    textos_saida = []\n",
        "\n",
        "    for texto in textos:\n",
        "        doc = nlp(\" \".join(texto)) \n",
        "\n",
        "        sentenca = []\n",
        "        for token in doc:\n",
        "          if token.pos_ in postags_permitidas:\n",
        "              sentenca.append(token.lemma_)\n",
        "\n",
        "        if len(sentenca) != 0:\n",
        "          textos_saida.append(sentenca)\n",
        "\n",
        "    return textos_saida"
      ],
      "metadata": {
        "id": "SbnNOPv85d0C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preparaCorpus"
      ],
      "metadata": {
        "id": "b32wPnBG1faQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import das biblitecas\n",
        "import pandas as pd\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "def preparaCorpus(textos,                   \n",
        "                  sentenciaTexto=False,\n",
        "                  tornaMinusculo=False,\n",
        "                  removePontuacao=False, \n",
        "                  removeStopwords=False, \n",
        "                  bigramas=False, \n",
        "                  trigramas=False,\n",
        "                  somenteRelevante=False,\n",
        "                  postag_relevante=['VERB', 'AUX', 'NOUN'],\n",
        "                  lematizar=False,                  \n",
        "                  postag_lema=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \n",
        "    # Verifica se é um textos é str ou uma lista de texto\n",
        "    if type(textos) is str:\n",
        "      # Sentencia o texto\n",
        "      lista_sentencas = [textos]\n",
        "    else:\n",
        "      lista_sentencas = textos\n",
        "    \n",
        "    # Converte o texto em uma lista de sentencas\n",
        "    if sentenciaTexto==True:\n",
        "      lista_sentencas = getSentencasTexto(lista_sentencas)\n",
        "\n",
        "    # Converte o texto em minúsuclo\n",
        "    if tornaMinusculo==True:\n",
        "      lista_sentencas = getSentencasMinusculo(lista_sentencas)\n",
        "    \n",
        "    # tokeniza o texto\n",
        "    lista_sentencas_palavras = getTokensTexto(lista_sentencas)\n",
        "\n",
        "    # Remove a pontuação \n",
        "    if removePontuacao==True:\n",
        "        lista_sentencas_palavras = removerPontuacao(lista_sentencas_palavras)        \n",
        "\n",
        "    # Remove as stop words\n",
        "    if removeStopwords==True:\n",
        "      lista_sentencas_palavras = getTokensSemStopword(lista_sentencas_palavras)\n",
        "\n",
        "    # Criar bigramas ou trigramas\n",
        "    if bigramas==True:\n",
        "      # Construa os modelos de bigramas\n",
        "      bigram = gensim.models.Phrases(lista_sentencas_palavras, min_count=5, threshold=100) # max_topicse mais alto menos frases.\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "      bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "      lista_sentencas_palavras = [bigram_mod[doc] for doc in lista_sentencas_palavras]\n",
        "    \n",
        "    if trigramas==True:      \n",
        "      # Construa os modelos de bigramas\n",
        "      bigram = gensim.models.Phrases(lista_sentencas_palavras, min_count=5, threshold=100) # max_topicse mais alto menos frases.\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "      bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "      # Construa os modelos de trigramas\n",
        "      trigram = gensim.models.Phrases(bigram[lista_sentencas_palavras], threshold=100)\n",
        "      # Maneira mais rápida de obter uma frase batida como um trigrama/bigrama    \n",
        "      trigram_mod = gensim.models.phrases.Phraser(trigram)   \n",
        "      lista_sentencas_palavras = [trigram_mod[bigram_mod[doc]] for doc in lista_sentencas_palavras]   \n",
        "    \n",
        "    # Somente palavras relevantes\n",
        "    if somenteRelevante==True:      \n",
        "      lista_sentencas_palavras = relevantes(lista_sentencas_palavras, postags_permitidas=postag_relevante)\n",
        "    \n",
        "    # Faça a lematização mantendo apenas para noun, adj, vb, adv\n",
        "    if lematizar==True:      \n",
        "      lista_sentencas_palavras = lematizacao(lista_sentencas_palavras, postags_permitidas=postag_lema)\n",
        "\n",
        "    return lista_sentencas_palavras"
      ],
      "metadata": {
        "id": "rSW4ign41h1L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPuCCLyuBIeZ"
      },
      "source": [
        "# Exemplos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Conjunto de dados\n"
      ],
      "metadata": {
        "id": "BZQFeIobYaYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9QEWUZkfiiwd"
      },
      "outputs": [],
      "source": [
        "CohQuAD_Coh = [\n",
        "# 20 Perguntas do CohQuAD Coerentes\n",
        "\"Como enfileirar elementos em uma fila?\",      \n",
        "\"Como desenfileirar elementos em uma fila?\",\n",
        "\"Como empilhar elementos em uma pilha?\",\n",
        "\"Como empilhar e desempilhar elementos em uma pilha?\",\n",
        "\"Como empilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"Como empilhar e desempilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"Como desempilhar elementos em uma pilha?\",\n",
        "\"Como desempilhar elementos em uma estrutura de dados pilha?\",\n",
        "\"O que é uma pilha e como empilhar seu elemento?\",\n",
        "\"O que é uma fila e como enfileirar seu elemento?\",\n",
        "\"O que é uma fila e como desenfileirar um elemento nela?\",\n",
        "\"O que é uma pilha e como desempilhar um elemento nela?\",\n",
        "\"O que é uma fila e como enfileirar um elemento nela?\",\n",
        "\"O que é uma pilha e como empilhar um elemento nela?\",\n",
        "\"O que é uma pilha e como empilhar e desempilhar seus elementos?\",\n",
        "\"O que é uma fila e como enfileirar e desenfileirar seus elementos?\",\n",
        "\"Como são implementadas as operações de empilhar e desempilhar elementos em uma pilha?\",\n",
        "\"Como são implementadas as operações de enfileirar e desenfileirar elementos em uma fila?\",\n",
        "\"Em uma pilha a operação de empilhar ocorre em qual extremidade?\",\n",
        "\"Em uma fila a operação de enfileirar ocorre em qual extremidade?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EwF293tUiiwi"
      },
      "outputs": [],
      "source": [
        "CohQuAD_Inc = [\n",
        "# 20 Perguntas do CohQuAD Incoerentes\n",
        "\"Como enfileirar elementos em uma pilha?\",\n",
        "\"Como desenfileirar elementos em uma pilha?\",\n",
        "\"Como empilhar elementos em uma fila?\",\n",
        "\"Como empilhar e desempilhar elementos em uma fila?\",\n",
        "\"Como empilhar elementos em uma estrutura de dados fila?\",\n",
        "\"Como empilhar e desempilhar elementos em uma estrutura de dados fila?\",\n",
        "\"Como desempilhar elementos em uma fila?\",\n",
        "\"Como desempilhar elementos em uma estrutura de dados fila?\",\n",
        "\"O que é uma fila e como empilhar seu elemento?\",\n",
        "\"O que é uma pilha e como enfileirar seu elemento?\",\n",
        "\"O que é uma pilha e como desenfileirar um elemento nela?\",\n",
        "\"O que é uma fila e como desempilhar um elemento nela?\",\n",
        "\"O que é uma pilha e como enfileirar um elemento nela?\",\n",
        "\"O que é uma fila e como empilhar um elemento nela?\",\n",
        "\"O que é uma fila e como empilhar e desempilhar seus elementos?\",\n",
        "\"O que é uma pilha e como enfileirar e desenfileirar seus elementos?\",\n",
        "\"Como são implementadas as operações de empilhar e desempilhar elementos em uma fila?\",\n",
        "\"Como são implementadas as operações de enfileirar e desenfileirar elementos em uma pilha?\",\n",
        "\"Em uma pilha a operação de enfileirar ocorre em qual extremidade?\",\n",
        "\"Em uma fila a operação de empilhar ocorre em qual extremidade?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando o dicionário sem lematização e sem as stopwords\n",
        "\n",
        "textos = preparaCorpus(CohQuAD_Coh,                        \n",
        "                       sentenciaTexto=True,\n",
        "                       tornaMinusculo=True,\n",
        "                       removePontuacao=True,\n",
        "                       somenteRelevante=False,\n",
        "                       removeStopwords=False)"
      ],
      "metadata": {
        "id": "aJecgsByjHDu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Cálculo de PMI"
      ],
      "metadata": {
        "id": "U02pkc1ueo60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 1"
      ],
      "metadata": {
        "id": "xQGqu8jXjXe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens = len(textos[0])\n",
        "print(n_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxnoFaT_rhKH",
        "outputId": "51d56aef-9b3a-4e36-c166-097be08dff23"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sem janela\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela\n",
        "def gerarBigramasJanela(data, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(data)):\n",
        "        janela = data[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama"
      ],
      "metadata": {
        "id": "a-246Qn3kc-5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textos_bgs = gerarBigramasJanela(textos[0])\n",
        "print(textos_bgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg2W1Eagks2m",
        "outputId": "43da4efd-89cf-4eda-96d5-c05cefb4108f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['como', 'enfileirar'], ['enfileirar', 'elementos'], ['elementos', 'em'], ['em', 'uma'], ['uma', 'fila']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contarFrequenciaPalavra(palavra, doc):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq"
      ],
      "metadata": {
        "id": "RJ88iAagqX_X"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_w1 = contarFrequenciaPalavra(doc=textos[0],palavra='como')\n",
        "print(n_w1)\n",
        "n_w2 = contarFrequenciaPalavra(doc=textos[0],palavra='enfileirar')\n",
        "print(n_w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcTYoAI2qdoe",
        "outputId": "8f6697d5-ae81-4c10-93c8-e59ef116dfd6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "        if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "            freq = freq + 1\n",
        "    return freq"
      ],
      "metadata": {
        "id": "-meBRm9CsUvt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_w1_w2 = contaFrequenciaBigrama(w1='como', w2='enfileirar',bigrams=textos_bgs)\n",
        "print(n_w1_w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8HSK08_j0Nf",
        "outputId": "0b45f217-24fa-4951-f4e9-ef697292ff22"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def pmi(P_x, P_y, P_xy):\n",
        "    return math.log2(P_xy / (P_x * P_y))\n",
        "\n",
        "p_w1 = probabilidade(n_w1, n_tokens)\n",
        "\n",
        "p_w2 = probabilidade(n_w2, n_tokens)\n",
        "\n",
        "p_w1_w2 = probabilidade(n_w1_w2, len(textos_bgs))\n",
        "\n",
        "r = pmi(p_w1, p_w2, p_w1_w2)\n",
        "\n",
        "print(\"pmi para como e enfileirar = \",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLNHw5Zgl433",
        "outputId": "ccba9167-1b11-46bc-fef3-64955286ec20"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pmi para como e enfileirar =  2.8479969065549504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 2"
      ],
      "metadata": {
        "id": "gze2XHNx3S4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Sem janela\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela\n",
        "def gerarBigramasJanela(doc, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(doc)):\n",
        "        janela = doc[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama\n",
        "\n",
        "\n",
        "def contarFrequenciaPalavra(doc,palavra):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq\n",
        "\n",
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "      if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "        freq = freq + 1\n",
        "    return freq\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def calculaPMI(w1, w2, doc, tamanho_janela=2):\n",
        "    \n",
        "    # Quantidade tokens do texto\n",
        "    n_tokens = len(doc)\n",
        "    # print(\"n_tokens:\",n_tokens)\n",
        "    \n",
        "    # Conta as frequencias de w1 e w2\n",
        "    n_w1 = contarFrequenciaPalavra(doc,w1)\n",
        "    # print(\"n_w1:\",n_w1)\n",
        "    n_w2 = contarFrequenciaPalavra(doc,w2)    \n",
        "    # print(\"n_w2:\",n_w2)\n",
        "\n",
        "    # Gera os bigrammas do documento\n",
        "    # bgs = gerarBigramas(doc)\n",
        "    bgs = gerarBigramasJanela(doc,tamanho_janela)\n",
        "\n",
        "    # Conta a frequencia no brigrama\n",
        "    n_w1_w2 = contaFrequenciaBigrama(w1, w2 , bigrams=bgs)\n",
        "    print(\"n_w1_w2:\",n_w1_w2)\n",
        "    \n",
        "    # Calcula as probabilidades\n",
        "    p_w1 = probabilidade(n_w1, n_tokens)\n",
        "    # print(\"p_w1:\",p_w1)\n",
        "    p_w2 = probabilidade(n_w2, n_tokens)\n",
        "    # print(\"p_w2:\",p_w2)\n",
        "    # p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    # print(\"p_w1_w2:\",p_w1_w2)\n",
        "    # Para evitar divisão por 0 e log de 0\n",
        "    e = sys.float_info.min\n",
        "\n",
        "    r = math.log2(((p_w1_w2+e) / ((p_w1 * p_w2) + e)))\n",
        "    \n",
        "    return r"
      ],
      "metadata": {
        "id": "sDsQZQ4pyviQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo pmi"
      ],
      "metadata": {
        "id": "_1dXCwz7zQo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'como'\n",
        "w2 = 'enfileirar'\n",
        "r = calculaPMI(w1, w2, textos[0])\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxFdneuCz-UK",
        "outputId": "d4ec8b6d-ad56-489d-c370-4900867d88b8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_w1_w2: 1\n",
            "pmi para 'como' e 'enfileirar' = 2.584962500721156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simetria"
      ],
      "metadata": {
        "id": "1Tg-lCbzzS-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'enfileirar'\n",
        "w2 = 'como'\n",
        "r = calculaPMI(w1, w2, textos[0])\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF0Ostl9DjSG",
        "outputId": "e6f89467-5baa-48e9-a4dd-8b4c4c03dcac"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_w1_w2: 1\n",
            "pmi para 'enfileirar' e 'como' = 2.584962500721156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculo igual a\n",
        "https://www.listendata.com/2022/06/pointwise-mutual-information-pmi.html"
      ],
      "metadata": {
        "id": "iSII2aSvzWZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dado = ['this','is','a','foo','bar','bar','black','sheep','foo','bar','bar','black','sheep','foo','bar','bar','black','sheep','shep','bar','bar','black','sentence']\n",
        "print(dado)\n",
        "\n",
        "w1 = 'foo'\n",
        "w2 = 'bar'\n",
        "\n",
        "r = calculaPMI(w1, w2, dado)\n",
        "\n",
        "print(\"pmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB878l99gfnx",
        "outputId": "0f6fc163-c221-4be2-ac4d-50fdd03e8d06"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'foo', 'bar', 'bar', 'black', 'sheep', 'foo', 'bar', 'bar', 'black', 'sheep', 'foo', 'bar', 'bar', 'black', 'sheep', 'shep', 'bar', 'bar', 'black', 'sentence']\n",
            "n_w1_w2: 3\n",
            "pmi para 'foo' e 'bar' = 1.5235619560570128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Cálculo de NPMI\n",
        "\n",
        "Intervalo [-1 , 1]"
      ],
      "metadata": {
        "id": "iMp8LD_v6jvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculo 1"
      ],
      "metadata": {
        "id": "A-21Uckv6jvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def gerarBigramas(doc):      \n",
        "  sentenca_bigrama = []\n",
        "  for n, k in enumerate(doc):       \n",
        "      if n < len(doc) - 1:\n",
        "        sentenca_bigrama.append([k, doc[n + 1]])\n",
        "\n",
        "  return sentenca_bigrama\n",
        "\n",
        "# Com janela\n",
        "def gerarBigramasJanela(doc, tamanho_janela=2):      \n",
        "    sentenca_bigrama = []\n",
        "    \n",
        "    for idx in range(len(doc)):\n",
        "        janela = doc[idx: idx + tamanho_janela]\n",
        "        \n",
        "        if len(janela) < 2:\n",
        "            break\n",
        "\n",
        "        palavra = janela[0]\n",
        "        for proxima_palavra in janela[1:]:\n",
        "            sentenca_bigrama.append([palavra, proxima_palavra])\n",
        "\n",
        "    return sentenca_bigrama\n",
        "\n",
        "def contarFrequenciaPalavra(doc, palavra):\n",
        "    freq = 0\n",
        "    for tok in doc:\n",
        "        if tok == palavra:\n",
        "            freq = freq + 1\n",
        "\n",
        "    return freq\n",
        "\n",
        "def contaFrequenciaBigrama(w1, w2, bigrams):\n",
        "    freq = 0\n",
        "    for bg in bigrams:\n",
        "        if (bg[0] == w1 and bg[1] == w2) or (bg[0] == w2 and bg[1] == w1):\n",
        "        # if (bg[0] == w1 and bg[1] == w2):\n",
        "          freq = freq + 1\n",
        "    return freq\n",
        "\n",
        "def probabilidade(x, n):\n",
        "    return x / n\n",
        "\n",
        "def calculaNPMI(w1, w2, doc, tamanho_janela=2):\n",
        "    \n",
        "   # Quantidade tokens do texto\n",
        "    n_tokens = len(doc)\n",
        "    # print(\"n_tokens:\",n_tokens)\n",
        "    \n",
        "    # Conta as frequencias de w1 e w2\n",
        "    n_w1 = contarFrequenciaPalavra(doc,w1)\n",
        "    # print(\"n_w1:\",n_w1)\n",
        "    n_w2 = contarFrequenciaPalavra(doc,w2)    \n",
        "    # print(\"n_w2:\",n_w2)\n",
        "\n",
        "    # Gera os bigrammas do documento\n",
        "    # bgs = gerarBigramas(doc)\n",
        "    bgs = gerarBigramasJanela(doc,tamanho_janela)\n",
        "\n",
        "    # Conta a frequencia no brigrama\n",
        "    n_w1_w2 = contaFrequenciaBigrama(w1, w2 , bigrams=bgs)\n",
        "    # print(\"n_w1_w2:\",n_w1_w2)\n",
        "\n",
        "    # Calcula as probabilidades\n",
        "    p_w1 = probabilidade(n_w1, n_tokens)\n",
        "    # print(\"p_w1:\",p_w1)\n",
        "    p_w2 = probabilidade(n_w2, n_tokens)\n",
        "    # print(\"p_w2:\",p_w2)\n",
        "    # p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    p_w1_w2 = probabilidade(n_w1_w2, n_tokens)\n",
        "    # print(\"p_w1_w2:\",p_w1_w2)\n",
        "    # Para evitar divisão por 0 e log de 0\n",
        "    e = sys.float_info.min\n",
        "\n",
        "    # Para evitar divisão por 0 e log de 0\n",
        "    e = sys.float_info.min\n",
        "    \n",
        "    r = math.log2( (p_w1_w2  + e) / ((p_w1 * p_w2) + e))\n",
        "\n",
        "    nr = r / -(math.log2(p_w1_w2 + e))\n",
        "    return nr"
      ],
      "metadata": {
        "id": "6_r45HT36jvW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'como'\n",
        "w2 = 'enfileirar'\n",
        "r = calculaNPMI(w1, w2, textos[0])\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22532d00-546e-4968-a4ff-5431f605327d",
        "id": "uJtR9-vi6jvX"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npmi para 'como' e 'enfileirar' = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'enfileirar'\n",
        "w2 = 'como'\n",
        "r = calculaNPMI(w2, w1, textos[0])\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWoueZ56DcSo",
        "outputId": "fd2fb61e-ddfb-44d1-ea7f-a6451d358751"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npmi para 'enfileirar' e 'como' = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = 'como'\n",
        "w2 = 'elementos'\n",
        "r = calculaNPMI(w1, w2, textos[0])\n",
        "\n",
        "print(\"npmi para '\"+ w1 +\"' e '\" + w2 + \"' =\",r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5w4J76aST4f",
        "outputId": "fa453945-3b72-4af6-ed8e-4f83e1b73370"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npmi para 'como' e 'elementos' = -0.9949413649692346\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/GPRan9UQRzb/e/ss+NzF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}